{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../data/input/groundtruth/'\n",
    "OUTPUT_DIR = '../data/input/groundtruth/'\n",
    "\n",
    "CORPUS_DIR = os.path.join(OUTPUT_DIR, 'corpus')\n",
    "VECTORIZED_DIR = os.path.join(OUTPUT_DIR, 'vectorized_trainset')\n",
    "\n",
    "text_col_header = 'text'\n",
    "label_col_header = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_train_file = os.path.join(INPUT_DIR, 'speechact_train.csv')\n",
    "df_train = pd.read_csv(ip_train_file)\n",
    "df_train = df_train.astype({text_col_header: str, label_col_header: int})\n",
    "\n",
    "ip_test_file = os.path.join(INPUT_DIR, 'speechact_test.csv')\n",
    "df_test = pd.read_csv(ip_test_file)\n",
    "df_test = df_test.astype({text_col_header: str, label_col_header: int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataframe (37099, 2)\n",
      "Counter({0: 15645, 1: 11629, 2: 9825})\n"
     ]
    }
   ],
   "source": [
    "df_train_augmented = pd.DataFrame(columns=[text_col_header, label_col_header])\n",
    "df_train_augmented = pd.concat([df_train_augmented, df_train], axis=0, sort=False)\n",
    "\n",
    "print('Augmented dataframe', df_train_augmented.shape)\n",
    "print(Counter(df_train_augmented[label_col_header]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read augmented data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (1331, 2)\n",
      "Only augmented dataframe shape: (1210, 2)\n",
      "Number of unique values: 1078\n",
      "Label size 1078\n",
      "                                                text  label\n",
      "0  a carnivore meaning a flesh eater in carnivoro...      0\n",
      "1  a carnivore meaning a flesh eater or eater of ...      0\n",
      "2  a carnivore meaning a flesh eater or eatery in...      0\n",
      "3  a carnivore meaning a flesh eater or eatery in...      0\n",
      "4  a carnivore meaning a flesh eater or eatery in...      0\n",
      "Augmented dataframe (38177, 2)\n",
      "Counter({0: 16723, 1: 11629, 2: 9825})\n"
     ]
    }
   ],
   "source": [
    "class_label = 0\n",
    "ip_aug_file = os.path.join(INPUT_DIR, 'gpt2/augmented_class'+str(class_label)+'_statement.csv')\n",
    "\n",
    "df_aug_class = pd.read_csv(ip_aug_file, sep='\\t')\n",
    "print('Dataframe shape:',df_aug_class.shape)\n",
    "\n",
    "df_aug_class = df_aug_class.loc[df_aug_class['is_agumented'] == 1]\n",
    "print('Only augmented dataframe shape:',df_aug_class.shape)\n",
    "\n",
    "text = np.unique(df_aug_class['text'].values)\n",
    "print('Number of unique values:', len(text))\n",
    "\n",
    "labels = [class_label] * len(text)\n",
    "print('Label size', len(labels))\n",
    "\n",
    "df_aug_class = pd.DataFrame(zip(text, labels), columns=[text_col_header, label_col_header])\n",
    "print(df_aug_class.head())\n",
    "\n",
    "df_train_augmented = pd.concat([df_train_augmented, df_aug_class], axis=0, sort=False)\n",
    "print('Augmented dataframe', df_train_augmented.shape)\n",
    "\n",
    "print(Counter(df_train_augmented[label_col_header]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (968, 2)\n",
      "Only augmented dataframe shape: (880, 2)\n",
      "Number of unique values: 718\n",
      "Label size 718\n",
      "                                text  label\n",
      "0         alien abductees of chicago      1\n",
      "1           alien abductees of china      1\n",
      "2  alien abductees of chinese origin      1\n",
      "3       alien abductees of hiroshima      1\n",
      "4     alien abductees of kurukshetra      1\n",
      "Augmented dataframe (38895, 2)\n",
      "Counter({0: 16723, 1: 12347, 2: 9825})\n"
     ]
    }
   ],
   "source": [
    "class_label = 1\n",
    "ip_aug_file = os.path.join(INPUT_DIR, 'gpt2/augmented_class'+str(class_label)+'_interrogative.csv')\n",
    "\n",
    "df_aug_class = pd.read_csv(ip_aug_file, sep='\\t')\n",
    "print('Dataframe shape:',df_aug_class.shape)\n",
    "\n",
    "df_aug_class = df_aug_class.loc[df_aug_class['is_agumented'] == 1]\n",
    "print('Only augmented dataframe shape:',df_aug_class.shape)\n",
    "\n",
    "text = np.unique(df_aug_class['text'].values)\n",
    "print('Number of unique values:', len(text))\n",
    "\n",
    "labels = [class_label] * len(text)\n",
    "print('Label size', len(labels))\n",
    "\n",
    "df_aug_class = pd.DataFrame(zip(text, labels), columns=[text_col_header, label_col_header])\n",
    "print(df_aug_class.head())\n",
    "\n",
    "df_train_augmented = pd.concat([df_train_augmented, df_aug_class], axis=0, sort=False)\n",
    "print('Augmented dataframe', df_train_augmented.shape)\n",
    "\n",
    "print(Counter(df_train_augmented[label_col_header]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (1474, 2)\n",
      "Only augmented dataframe shape: (1340, 2)\n",
      "Number of unique values: 874\n",
      "Label size 874\n",
      "                                                text  label\n",
      "0  actually not clear on the exact content of the...      2\n",
      "1  actually not clear that such an article would ...      2\n",
      "2  actually not clear that the corresponding nota...      2\n",
      "3  actually not clear that the coverage that we h...      2\n",
      "4  actually not clear that the inclusion criteria...      2\n",
      "Augmented dataframe (39769, 2)\n",
      "Counter({0: 16723, 1: 12347, 2: 10699})\n"
     ]
    }
   ],
   "source": [
    "class_label = 2\n",
    "ip_aug_file = os.path.join(INPUT_DIR, 'gpt2/augmented_class'+str(class_label)+'_imperative.csv')\n",
    "\n",
    "df_aug_class = pd.read_csv(ip_aug_file, sep='\\t')\n",
    "print('Dataframe shape:',df_aug_class.shape)\n",
    "\n",
    "df_aug_class = df_aug_class.loc[df_aug_class['is_agumented'] == 1]\n",
    "print('Only augmented dataframe shape:',df_aug_class.shape)\n",
    "\n",
    "text = np.unique(df_aug_class['text'].values)\n",
    "print('Number of unique values:', len(text))\n",
    "\n",
    "labels = [class_label] * len(text)\n",
    "print('Label size', len(labels))\n",
    "\n",
    "df_aug_class = pd.DataFrame(zip(text, labels), columns=[text_col_header, label_col_header])\n",
    "print(df_aug_class.head())\n",
    "\n",
    "df_train_augmented = pd.concat([df_train_augmented, df_aug_class], axis=0, sort=False)\n",
    "print('Augmented dataframe', df_train_augmented.shape)\n",
    "\n",
    "print(Counter(df_train_augmented[label_col_header]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_file = os.path.join(OUTPUT_DIR,'speechact_augmented_gpt2_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_augmented.to_csv(op_file, index=False, header=[text_col_header, label_col_header])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the data is augmented text train file is available, run from the below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_augmented = pd.read_csv(op_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37099, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_augmented.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39769, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_stopwords=False):\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # print(text, len(text), end ='\\n')\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W',' ', text)\n",
    "    text = re.sub(' \\d+', ' ', text)\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        words = text.split(' ')\n",
    "        words = [w.strip() for w in words if w not in stopwords.words('english')]\n",
    "        text = ' '.join(words)\n",
    "    text = text.strip()\n",
    "        \n",
    "    # print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_based_on_occurences(text_list, min_occurences=1):\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    alltext = ''\n",
    "    for text in text_list:\n",
    "        alltext += text\n",
    "        \n",
    "    wordlist = alltext.split()\n",
    "\n",
    "    word_freq = dict(Counter(wordlist))\n",
    "    # print(word_freq)\n",
    "    \n",
    "    for word,freq in word_freq.items():\n",
    "        if (freq >= min_occurences):\n",
    "            corpus.append(word)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df_data, text_col_header, remove_stopwords):\n",
    "    \n",
    "    df_data[text_col_header] = df_data[text_col_header].apply(lambda x: preprocess_text(x,remove_stopwords))\n",
    "    df_data.drop(df_data[df_data[text_col_header] == ''].index, inplace=True)\n",
    "    \n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating multiple corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Length  39769\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords = False\n",
    "text_col_header = 'text'\n",
    "\n",
    "df_train_stopword = df_train.copy()\n",
    "df_train_stopword = preprocess_df(df_train_stopword, text_col_header, remove_stopwords)\n",
    "\n",
    "# df_test_stopword = df_test.copy()\n",
    "# df_test_stopword = preprocess_df(df_test_stopword, text_col_header, remove_stopwords)\n",
    "\n",
    "corpus_stopword = df_train_stopword[text_col_header].values\n",
    "\n",
    "print('Corpus Length ', len(corpus_stopword))\n",
    "\n",
    "corpus_file = os.path.join(CORPUS_DIR, 'aug_gpt2_corpus_stopword.pkl')\n",
    "with open(corpus_file, 'wb') as f_op:\n",
    "    pkl.dump(corpus_stopword, f_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Length  39688\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords = True\n",
    "text_col_header = 'text'\n",
    "\n",
    "df_train_nostopword = df_train.copy()\n",
    "df_train_nostopword = preprocess_df(df_train_nostopword, text_col_header, remove_stopwords)\n",
    "\n",
    "# df_test_nostopword = df_test.copy()\n",
    "# df_test_nostopword = preprocess_df(df_test_nostopword, text_col_header, remove_stopwords)\n",
    "\n",
    "corpus_nostopword = df_train_nostopword[text_col_header].values\n",
    "\n",
    "print('Corpus Length ', len(corpus_nostopword))\n",
    "\n",
    "corpus_file = os.path.join(CORPUS_DIR, 'aug_gpt2_corpus_nostopword.pkl')\n",
    "with open(corpus_file, 'wb') as f_op:\n",
    "    pkl.dump(corpus_nostopword, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create optional corpus with only minimum occurring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_min_occurrences = remove_words_based_on_occurences(corpus_nostopword, min_occurences=2)\n",
    "\n",
    "corpus_file = os.path.join(CORPUS_DIR, 'aug_gpt2_corpus_min_occurrences_2.pkl')\n",
    "with open(corpus_file, 'wb') as f_op:\n",
    "    pkl.dump(corpus_min_occurrences, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization - Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are 3 corpus that are to be vectorized\n",
    "  <li> Bag of words </li>\n",
    "  <li> Tf-IDF </li>\n",
    "  <li> Word embeddings - Glove </li>\n",
    "  <li> BERT embeddings </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data train: (39769, 34654)\n",
      "Shape of the label train: (39769, 1)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_stopword)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'aug_gpt2_vector_countvector_stopword.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'aug_gpt2_train_data_countvector_stopword.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'aug_gpt2_train_label_countvector_stopword.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)\n",
    "\n",
    "# data_test = vectorizer.transform(df_test_stopword[text_col_header])\n",
    "# print('Shape of the data train:',data_test.shape)\n",
    "# label_test = np.array(df_test_stopword[label_col_header])\n",
    "# label_test = label_test.reshape((len(label_test), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_nostopword)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'aug_gpt2_vector_countvector_nostopword.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'aug_gpt2_train_data_countvector_nostopword.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'aug_gpt2_train_label_countvector_nostopword.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - minimum occurrence words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_min_occurrences)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'aug_gpt2_vector_countvector_minoccurences_2.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'aug_gpt2_train_data_countvector_minoccurences.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'aug_gpt2_train_label_countvector_minoccurences.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus_stopword)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'aug_gpt2_vector_tfidfvector_stopword.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'aug_gpt2_train_data_tfidfvector_stopword.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'aug_gpt2_train_label_tfidfvector_stopword.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus_nostopword)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'aug_gpt2_vector_tfidfvector_nostopword.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'aug_gpt2_train_data_tfidfvector_nostopword.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'aug_gpt2_train_label_tfidfvector_nostopword.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - minimum occurrence words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus_min_occurrences)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'aug_gpt2_vector_tfidfvector_minoccurences_2.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'aug_gpt2_train_data_tfidfvector_minoccurences.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'aug_gpt2_train_label_tfidfvector_minoccurences.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentgen",
   "language": "python",
   "name": "sentgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
