{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../data/input/groundtruth/'\n",
    "OUTPUT_DIR = '../data/input/groundtruth/'\n",
    "\n",
    "CORPUS_DIR = os.path.join(OUTPUT_DIR, 'corpus')\n",
    "VECTORIZED_DIR = os.path.join(OUTPUT_DIR, 'vectorized_trainset')\n",
    "\n",
    "text_col_header = 'text'\n",
    "label_col_header = 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading test and train data to keep them ready for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read Train data\n",
    "ip_train_file = os.path.join(INPUT_DIR, 'speechact_augmented_train.csv')\n",
    "df_train = pd.read_csv(ip_train_file)\n",
    "df_train = df_train.astype({text_col_header: str, label_col_header: int})\n",
    "\n",
    "# Read Test data\n",
    "ip_test_file = os.path.join(INPUT_DIR, 'speechact_test.csv')\n",
    "df_test = pd.read_csv(ip_test_file)\n",
    "df_test = df_test.astype({text_col_header: str, label_col_header: int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4268921, 2) (9275, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "<li> Input - Vectorized Data train </li>\n",
    "<li> Input - Vectorized Label train </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter Vector with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vectorized train data and train label\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'aug_train_data_countvector_stopword.pkl')\n",
    "with open(vectorized_train_data_file, 'rb') as f_ip:\n",
    "    data_train = pkl.load(f_ip)\n",
    "    \n",
    "train_label_file = os.path.join(VECTORIZED_DIR, 'aug_train_label_countvector_stopword.pkl')\n",
    "with open(train_label_file, 'rb') as f_ip:\n",
    "    label_train = pkl.load(f_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4268921, 61054) (4268921, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape, label_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vectorizer to transform the test data\n",
    "vector_file = os.path.join(CORPUS_DIR, 'aug_vector_countvector_stopword.pkl')\n",
    "with open(vector_file, 'rb') as f_ip:\n",
    "    vectorizer = pkl.load(f_ip)\n",
    "\n",
    "data_test = vectorizer.transform(df_test[text_col_header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Logistic Regression\n",
    "estimator = LogisticRegression(max_iter=1000)\n",
    "estimator.fit(data_train, np.ravel(label_train, order='C'))\n",
    "predictions_test = estimator.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9971245661374385\n",
      "Test Accuracy 0.9236657681940701\n",
      "F1 macro Score:  0.9228482386941913\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      3911\n",
      "           1       0.92      0.92      0.92      2908\n",
      "           2       0.90      0.93      0.92      2456\n",
      "\n",
      "    accuracy                           0.92      9275\n",
      "   macro avg       0.92      0.92      0.92      9275\n",
      "weighted avg       0.92      0.92      0.92      9275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "## Accuracy Measure\n",
    "print('Train Accuracy', estimator.score(data_train, df_train[label_col_header]))\n",
    "print('Test Accuracy', estimator.score(data_test, df_test[label_col_header]))\n",
    "\n",
    "# F1\n",
    "f1_measure = f1_score(df_test[label_col_header], predictions_test, average='macro')\n",
    "print('F1 macro Score: ', f1_measure)\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_test[label_col_header], predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter Vector with no stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vectorized train data and train label\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_countvector_nostopword.pkl')\n",
    "with open(vectorized_train_data_file, 'rb') as f_ip:\n",
    "    data_train = pkl.load(f_ip)\n",
    "    \n",
    "train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_countvector_nostopword.pkl')\n",
    "with open(train_label_file, 'rb') as f_ip:\n",
    "    label_train = pkl.load(f_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vectorizer to transform the test data\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_countvector_nostopword.pkl')\n",
    "with open(vector_file, 'rb') as f_ip:\n",
    "    vectorizer = pkl.load(f_ip)\n",
    "\n",
    "data_test = vectorizer.transform(df_test[text_col_header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Logistic Regression\n",
    "estimator = LogisticRegression(max_iter=1000)\n",
    "estimator.fit(data_train, np.ravel(label_train, order='C'))\n",
    "predictions_test = estimator.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9605380198927195\n",
      "Test Accuracy 0.8659838274932614\n",
      "F1 macro Score:  0.8661387567725706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89      3911\n",
      "           1       0.79      0.85      0.82      2908\n",
      "           2       0.91      0.88      0.89      2456\n",
      "\n",
      "    accuracy                           0.87      9275\n",
      "   macro avg       0.87      0.87      0.87      9275\n",
      "weighted avg       0.87      0.87      0.87      9275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "## Accuracy Measure\n",
    "print('Train Accuracy', estimator.score(data_train, df_train[label_col_header]))\n",
    "print('Test Accuracy', estimator.score(data_test, df_test[label_col_header]))\n",
    "\n",
    "# F1\n",
    "f1_measure = f1_score(df_test[label_col_header], predictions_test, average='macro')\n",
    "print('F1 macro Score: ', f1_measure)\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_test[label_col_header], predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter Vector with minimum occurences of words (min_occurences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vectorized train data and train label\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_countvector_minoccurences_2.pkl')\n",
    "with open(vectorized_train_data_file, 'rb') as f_ip:\n",
    "    data_train = pkl.load(f_ip)\n",
    "    \n",
    "train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_countvector_minoccurences_2.pkl')\n",
    "with open(train_label_file, 'rb') as f_ip:\n",
    "    label_train = pkl.load(f_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vectorizer to transform the test data\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_countvector_minoccurences_2.pkl')\n",
    "with open(vector_file, 'rb') as f_ip:\n",
    "    vectorizer = pkl.load(f_ip)\n",
    "\n",
    "data_test = vectorizer.transform(df_test[text_col_header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Logistic Regression\n",
    "estimator = LogisticRegression(max_iter=1000)\n",
    "estimator.fit(data_train, np.ravel(label_train, order='C'))\n",
    "predictions_test = estimator.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9453624086902612\n",
      "Test Accuracy 0.8636118598382749\n",
      "F1 macro Score:  0.863375437946586\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89      3911\n",
      "           1       0.78      0.85      0.81      2908\n",
      "           2       0.90      0.87      0.89      2456\n",
      "\n",
      "    accuracy                           0.86      9275\n",
      "   macro avg       0.86      0.86      0.86      9275\n",
      "weighted avg       0.87      0.86      0.86      9275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "## Accuracy Measure\n",
    "print('Train Accuracy', estimator.score(data_train, df_train[label_col_header]))\n",
    "print('Test Accuracy', estimator.score(data_test, df_test[label_col_header]))\n",
    "\n",
    "# F1\n",
    "f1_measure = f1_score(df_test[label_col_header], predictions_test, average='macro')\n",
    "print('F1 macro Score: ', f1_measure)\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_test[label_col_header], predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentgen",
   "language": "python",
   "name": "sentgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
