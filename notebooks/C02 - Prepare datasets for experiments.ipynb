{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../data/input/groundtruth/'\n",
    "OUTPUT_DIR = '../data/input/groundtruth/'\n",
    "\n",
    "CORPUS_DIR = os.path.join(OUTPUT_DIR, 'corpus')\n",
    "VECTORIZED_DIR = os.path.join(OUTPUT_DIR, 'vectorized_trainset')\n",
    "\n",
    "text_col_header = 'text'\n",
    "label_col_header = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_train_file = os.path.join(INPUT_DIR, 'speechact_train.csv')\n",
    "df_train = pd.read_csv(ip_train_file)\n",
    "df_train = df_train.astype({text_col_header: str, label_col_header: int})\n",
    "\n",
    "ip_test_file = os.path.join(INPUT_DIR, 'speechact_test.csv')\n",
    "df_test = pd.read_csv(ip_test_file)\n",
    "df_test = df_test.astype({text_col_header: str, label_col_header: int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketers representing industry raid manufactu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rolling stone magazine said richards had creat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>most designs are inflated through pyrotechnic ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>please focus on the article s topic not the title</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other types of arthropod produce silk most not...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  marketers representing industry raid manufactu...      0\n",
       "1  rolling stone magazine said richards had creat...      0\n",
       "2  most designs are inflated through pyrotechnic ...      0\n",
       "3  please focus on the article s topic not the title      2\n",
       "4  other types of arthropod produce silk most not...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37099 entries, 0 to 37098\n",
      "Data columns (total 2 columns):\n",
      "text     37099 non-null object\n",
      "label    37099 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 579.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there are many types of waxing suitable for re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>give the article a chance</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gimme a couple hours and ill nom it myself if ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in roman catholicism the baptism of jesus is o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>see notability</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  there are many types of waxing suitable for re...      0\n",
       "1                          give the article a chance      2\n",
       "2  gimme a couple hours and ill nom it myself if ...      2\n",
       "3  in roman catholicism the baptism of jesus is o...      0\n",
       "4                                     see notability      2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_stopwords=False):\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # print(text, len(text), end ='\\n')\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W',' ', text)\n",
    "    text = re.sub(' \\d+', ' ', text)\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        words = text.split(' ')\n",
    "        words = [w.strip() for w in words if w not in stopwords.words('english')]\n",
    "        text = ' '.join(words)\n",
    "    text = text.strip()\n",
    "        \n",
    "    # print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_based_on_occurences(text_list, min_occurences=1):\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    alltext = ''\n",
    "    for text in text_list:\n",
    "        alltext += text\n",
    "        \n",
    "    wordlist = alltext.split()\n",
    "\n",
    "    word_freq = dict(Counter(wordlist))\n",
    "    # print(word_freq)\n",
    "    \n",
    "    for word,freq in word_freq.items():\n",
    "        if (freq >= min_occurences):\n",
    "            corpus.append(word)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df_data, text_col_header, remove_stopwords):\n",
    "    \n",
    "    df_data[text_col_header] = df_data[text_col_header].apply(lambda x: preprocess_text(x,remove_stopwords))\n",
    "    df_data.drop(df_data[df_data[text_col_header] == ''].index, inplace=True)\n",
    "    \n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating multiple corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Length  37099\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords = False\n",
    "text_col_header = 'text'\n",
    "\n",
    "df_train_stopword = df_train.copy()\n",
    "df_train_stopword = preprocess_df(df_train_stopword, text_col_header, remove_stopwords)\n",
    "\n",
    "# df_test_stopword = df_test.copy()\n",
    "# df_test_stopword = preprocess_df(df_test_stopword, text_col_header, remove_stopwords)\n",
    "\n",
    "corpus_stopword = df_train_stopword[text_col_header].values\n",
    "\n",
    "print('Corpus Length ', len(corpus_stopword))\n",
    "\n",
    "corpus_file = os.path.join(CORPUS_DIR, 'corpus_stopword.pkl')\n",
    "with open(corpus_file, 'wb') as f_op:\n",
    "    pkl.dump(corpus_stopword, f_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Length  37020\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords = True\n",
    "text_col_header = 'text'\n",
    "\n",
    "df_train_nostopword = df_train.copy()\n",
    "df_train_nostopword = preprocess_df(df_train_nostopword, text_col_header, remove_stopwords)\n",
    "\n",
    "# df_test_nostopword = df_test.copy()\n",
    "# df_test_nostopword = preprocess_df(df_test_nostopword, text_col_header, remove_stopwords)\n",
    "\n",
    "corpus_nostopword = df_train_nostopword[text_col_header].values\n",
    "\n",
    "print('Corpus Length ', len(corpus_nostopword))\n",
    "\n",
    "corpus_file = os.path.join(CORPUS_DIR, 'corpus_nostopword.pkl')\n",
    "with open(corpus_file, 'wb') as f_op:\n",
    "    pkl.dump(corpus_nostopword, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create optional corpus with only minimum occurring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_min_occurrences = remove_words_based_on_occurences(corpus_nostopword, min_occurences=2)\n",
    "\n",
    "corpus_file = os.path.join(CORPUS_DIR, 'corpus_min_occurrences_2.pkl')\n",
    "with open(corpus_file, 'wb') as f_op:\n",
    "    pkl.dump(corpus_min_occurrences, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization - Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are 3 corpus that are to be vectorized\n",
    "  <li> Bag of words </li>\n",
    "  <li> Tf-IDF </li>\n",
    "  <li> Word embeddings - Glove </li>\n",
    "  <li> BERT embeddings </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data train: (37099, 34154)\n",
      "Shape of the label train: (37099, 1)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_stopword)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_countvector_stopword.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_countvector_stopword.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_countvector_stopword.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)\n",
    "\n",
    "# data_test = vectorizer.transform(df_test_stopword[text_col_header])\n",
    "# print('Shape of the data train:',data_test.shape)\n",
    "# label_test = np.array(df_test_stopword[label_col_header])\n",
    "# label_test = label_test.reshape((len(label_test), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data train: (37099, 34014)\n",
      "Shape of the label train: (37099, 1)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_nostopword)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_countvector_nostopword.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_countvector_nostopword.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_countvector_nostopword.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - minimum occurrence words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data train: (37099, 16518)\n",
      "Shape of the label train: (37099, 1)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_min_occurrences)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_countvector_minoccurences_2.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_countvector_minoccurences.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_countvector_minoccurences.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data train: (37099, 34154)\n",
      "Shape of the label train: (37099, 1)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus_stopword)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_tfidfvector_stopword.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_tfidfvector_stopword.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_tfidfvector_stopword.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data train: (37099, 34014)\n",
      "Shape of the label train: (37099, 1)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus_nostopword)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_tfidfvector_nostopword.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_tfidfvector_nostopword.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_tfidfvector_nostopword.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - minimum occurrence words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data train: (37099, 16518)\n",
      "Shape of the label train: (37099, 1)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus_min_occurrences)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_tfidfvector_minoccurences_2.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_tfidfvector_minoccurences.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_tfidfvector_minoccurences.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentgen",
   "language": "python",
   "name": "sentgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
