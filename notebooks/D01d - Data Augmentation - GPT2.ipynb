{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpt-2-simple in /Users/talat/anaconda3/lib/python3.6/site-packages (0.7.1)\n",
      "Requirement already satisfied: numpy in /Users/talat/.local/lib/python3.6/site-packages (from gpt-2-simple) (1.17.2)\n",
      "Requirement already satisfied: requests in /Users/talat/anaconda3/lib/python3.6/site-packages (from gpt-2-simple) (2.23.0)\n",
      "Requirement already satisfied: regex in /Users/talat/.local/lib/python3.6/site-packages (from gpt-2-simple) (2018.2.21)\n",
      "Requirement already satisfied: tqdm in /Users/talat/anaconda3/lib/python3.6/site-packages (from gpt-2-simple) (4.45.0)\n",
      "Requirement already satisfied: toposort in /Users/talat/anaconda3/lib/python3.6/site-packages (from gpt-2-simple) (1.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/talat/anaconda3/lib/python3.6/site-packages (from requests->gpt-2-simple) (1.22)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/talat/anaconda3/lib/python3.6/site-packages (from requests->gpt-2-simple) (2.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/talat/anaconda3/lib/python3.6/site-packages (from requests->gpt-2-simple) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/talat/anaconda3/lib/python3.6/site-packages (from requests->gpt-2-simple) (2018.4.16)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gpt-2-simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gpt_2_simple as gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading {model_name} model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 342Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 4.48Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 366Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:17, 28.6Mit/s]                                  \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 353Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 6.15Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 6.27Mit/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "model_name = \"124M\"\n",
    "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "    print(\"Downloading {model_name} model...\")\n",
    "    gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/talat/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 338025 tokens\n",
      "Training...\n",
      "[1 | 38.82] loss=4.11 avg=4.11\n",
      "[2 | 70.35] loss=3.95 avg=4.03\n",
      "[3 | 101.94] loss=3.78 avg=3.94\n",
      "[4 | 133.52] loss=3.70 avg=3.88\n",
      "[5 | 165.39] loss=3.78 avg=3.86\n",
      "[6 | 206.57] loss=3.58 avg=3.81\n",
      "[7 | 244.54] loss=3.38 avg=3.75\n",
      "[8 | 281.70] loss=3.68 avg=3.74\n",
      "[9 | 320.93] loss=3.69 avg=3.73\n",
      "[10 | 359.45] loss=3.72 avg=3.73\n",
      "[11 | 400.81] loss=3.77 avg=3.73\n",
      "[12 | 441.27] loss=3.46 avg=3.71\n",
      "[13 | 480.27] loss=3.66 avg=3.71\n",
      "[14 | 519.84] loss=3.52 avg=3.69\n",
      "[15 | 559.99] loss=3.61 avg=3.69\n",
      "[16 | 598.47] loss=3.77 avg=3.69\n",
      "[17 | 638.29] loss=3.70 avg=3.69\n",
      "[18 | 678.02] loss=3.44 avg=3.68\n",
      "[19 | 717.00] loss=3.48 avg=3.67\n",
      "[20 | 759.53] loss=3.48 avg=3.66\n",
      "[21 | 797.62] loss=3.41 avg=3.64\n",
      "[22 | 836.81] loss=3.39 avg=3.63\n",
      "[23 | 876.81] loss=3.38 avg=3.62\n",
      "[24 | 915.51] loss=3.47 avg=3.61\n",
      "[25 | 954.60] loss=3.48 avg=3.61\n",
      "[26 | 996.34] loss=3.69 avg=3.61\n",
      "[27 | 1035.55] loss=3.51 avg=3.60\n",
      "[28 | 1073.86] loss=3.28 avg=3.59\n",
      "[29 | 1112.31] loss=3.51 avg=3.59\n",
      "[30 | 1153.26] loss=3.49 avg=3.58\n",
      "[31 | 1191.60] loss=3.37 avg=3.58\n",
      "[32 | 1232.18] loss=3.13 avg=3.56\n",
      "[33 | 1270.94] loss=3.49 avg=3.56\n",
      "[34 | 1310.38] loss=3.48 avg=3.55\n",
      "[35 | 1349.24] loss=3.50 avg=3.55\n",
      "[36 | 1388.52] loss=3.55 avg=3.55\n",
      "[37 | 1428.43] loss=3.38 avg=3.55\n",
      "[38 | 1467.34] loss=3.32 avg=3.54\n",
      "[39 | 1507.18] loss=3.07 avg=3.53\n",
      "[40 | 1546.43] loss=3.40 avg=3.52\n",
      "[41 | 1585.26] loss=3.66 avg=3.53\n",
      "[42 | 1623.96] loss=3.31 avg=3.52\n",
      "[43 | 1663.71] loss=3.34 avg=3.51\n",
      "[44 | 1701.83] loss=3.39 avg=3.51\n",
      "[45 | 1740.97] loss=3.44 avg=3.51\n",
      "[46 | 1779.99] loss=3.31 avg=3.50\n",
      "[47 | 1818.57] loss=3.00 avg=3.49\n",
      "[48 | 1857.10] loss=3.39 avg=3.49\n",
      "[49 | 1896.64] loss=3.43 avg=3.49\n",
      "[50 | 1935.61] loss=3.49 avg=3.49\n",
      "[51 | 1974.70] loss=3.55 avg=3.49\n",
      "[52 | 2012.75] loss=3.30 avg=3.48\n",
      "[53 | 2051.95] loss=3.31 avg=3.48\n",
      "[54 | 2090.99] loss=3.40 avg=3.48\n",
      "[55 | 2129.98] loss=3.17 avg=3.47\n",
      "[56 | 2169.72] loss=3.12 avg=3.46\n",
      "[57 | 2208.93] loss=3.36 avg=3.46\n",
      "[58 | 2248.60] loss=3.30 avg=3.46\n",
      "[59 | 2286.19] loss=3.41 avg=3.45\n",
      "[60 | 2325.02] loss=3.20 avg=3.45\n",
      "[61 | 2364.76] loss=3.32 avg=3.45\n",
      "[62 | 2402.88] loss=3.02 avg=3.44\n",
      "[63 | 2441.84] loss=3.24 avg=3.43\n",
      "[64 | 2482.99] loss=3.16 avg=3.43\n",
      "[65 | 2520.65] loss=3.20 avg=3.42\n",
      "[66 | 2561.27] loss=3.18 avg=3.42\n",
      "[67 | 2598.17] loss=3.12 avg=3.41\n",
      "[68 | 2636.61] loss=3.17 avg=3.41\n",
      "[69 | 2675.62] loss=3.26 avg=3.40\n",
      "[70 | 2714.38] loss=3.08 avg=3.40\n",
      "[71 | 2752.60] loss=3.43 avg=3.40\n",
      "[72 | 2793.45] loss=3.08 avg=3.39\n",
      "[73 | 2831.83] loss=3.48 avg=3.39\n",
      "[74 | 2870.85] loss=3.11 avg=3.39\n",
      "[75 | 2908.19] loss=3.01 avg=3.38\n",
      "[76 | 2947.02] loss=3.16 avg=3.38\n",
      "[77 | 2985.87] loss=3.19 avg=3.37\n",
      "[78 | 3024.53] loss=3.41 avg=3.37\n",
      "[79 | 3063.04] loss=3.25 avg=3.37\n",
      "[80 | 3101.37] loss=3.33 avg=3.37\n",
      "[81 | 3141.70] loss=3.41 avg=3.37\n",
      "[82 | 3180.09] loss=3.04 avg=3.37\n",
      "[83 | 3216.90] loss=3.24 avg=3.36\n",
      "[84 | 3255.38] loss=3.35 avg=3.36\n",
      "[85 | 3294.06] loss=3.05 avg=3.36\n",
      "[86 | 3331.64] loss=3.37 avg=3.36\n",
      "[87 | 3370.15] loss=3.40 avg=3.36\n",
      "[88 | 3409.82] loss=3.07 avg=3.35\n",
      "[89 | 3447.83] loss=3.24 avg=3.35\n",
      "[90 | 3486.10] loss=3.29 avg=3.35\n",
      "[91 | 3524.63] loss=3.07 avg=3.35\n",
      "[92 | 3562.47] loss=3.04 avg=3.34\n",
      "[93 | 3601.87] loss=3.01 avg=3.34\n",
      "[94 | 3639.95] loss=3.24 avg=3.33\n",
      "[95 | 3677.80] loss=3.04 avg=3.33\n",
      "[96 | 3718.42] loss=3.11 avg=3.33\n",
      "[97 | 3757.26] loss=3.19 avg=3.32\n",
      "[98 | 3793.71] loss=3.13 avg=3.32\n",
      "[99 | 3832.77] loss=3.31 avg=3.32\n",
      "[100 | 3871.66] loss=3.16 avg=3.32\n",
      "======== SAMPLE 1 ========\n",
      ", your hearts I will not deny:\n",
      "That you with all your hearts\n",
      "Could have your thoughts and desires so soon,\n",
      "Were I to live by the very gates\n",
      "Of my heart, and by those that have not so\n",
      "Whom I love, by those that love me\n",
      "My true love, or by those whom I know not,\n",
      "As I will by that love be no more:\n",
      "For, from that love to my name,\n",
      "My true love and true love,\n",
      "And true love to mine heart, all is in vain\n",
      "That I love myself and my life, love to myself\n",
      "And the life of another\n",
      "But love a dead man dead. But love life\n",
      "So that there is no love to any dead man,\n",
      "If it be in the body of my dead man.\n",
      "\n",
      "KING RICHARD III:\n",
      "I now have told a lie,\n",
      "That I love my name\n",
      "Like the name of my living body\n",
      "A thousand times less dear, more holy, more holy,\n",
      "Than to say that God calls them\n",
      "And all who live, to say that I love my name\n",
      "Like the name of their living body\n",
      "A thousand times fewer holy, fewer holy,\n",
      "Than to say that I am a living spirit\n",
      "Of your heaven, your earth, and all of your people and of all\n",
      "All living men; to lie that I love their names\n",
      "Like gods and all the most holy\n",
      "So that I can give you my heavenly name,\n",
      "When living things have my name and I am their name,\n",
      "I love them as I love my gods, which name is\n",
      "Anointed as they are gods and I am their gods,\n",
      "And all these were but anointed\n",
      "For my very living glory and my living life.\n",
      "\n",
      "PARIS:\n",
      "My name is Peter:\n",
      "And your name is mine name; where art you born?\n",
      "\n",
      "KING RICHARD III:\n",
      "My name is yours on this land; with thy name\n",
      "And thy name on mine land. God bless you!\n",
      "If my name do me wrong, what will mine name\n",
      "Do? God bless me!\n",
      "\n",
      "PARIS:\n",
      "Your name to a thousand ways!\n",
      "\n",
      "KING RICHARD III:\n",
      "God bless, you! I am your name.\n",
      "\n",
      "PARIS:\n",
      "God bless, you, and your name to the name where my name\n",
      "Is.\n",
      "\n",
      "KING RICHARD III:\n",
      "You have my name, your name; wherefore I keep it; wherefore my name\n",
      "May have no name at all.\n",
      "\n",
      "PARIS:\n",
      "My name is, my name!--\n",
      "\n",
      "KING RICHARD III:\n",
      "God bless, you!\n",
      "What name is mine when my name is the name\n",
      "Of the world? what name is it when I\n",
      "My name is the name of the world? What name is it\n",
      "when my name is the name of the world? What name\n",
      "I am when my name is the name of the world?\n",
      "What name is the name of my father when\n",
      "My name is the name of my father?\n",
      "What name\n",
      "when my name is the name of the world? Wherefore\n",
      "What name\n",
      "are you my name?\n",
      "And what name is mine when my name is the name\n",
      "Of the world?\n",
      "God bless you, for I speak you my name. I give you\n",
      "The name, the name, the name.\n",
      "What name is that you give unto it:\n",
      "Which name you give unto an earth, a\n",
      "shy meadow, whose name is the name of the earth? Wherefore\n",
      "Why is my name the name of heaven?\n",
      "Wherefore your name is heaven, which in your name\n",
      "Gives itself the name of your earth? Wherefore your name\n",
      "I give name to earth. Is thy name mine? is that God?\n",
      "God bless you,--God bless thee!\n",
      "\n",
      "PARIS:\n",
      "I did not take it for granted--I thought I had,\n",
      "But I did give it up. You shall not believe\n",
      "That I make God my name. What have I given thee--\n",
      "The name, the name,--\n",
      "I give thee.\n",
      "God bless you, for I do swear by it.\n",
      "The name of my name,--God bless thee!\n",
      "\n",
      "MONTESTER:\n",
      "God bless you! I give you your name;--I give thee\n",
      "A name unto heaven, which I give to hell.\n",
      "If thou art the god of souls, God bless thee;\n",
      "I gave thee a name unto the heaven,\n",
      "That I give unto the hell.\n",
      "\n",
      "KING RICHARD III:\n",
      "You mean to say that I gave my name to heaven\n",
      "For hell's sake? If so, you have forfeited it!\n",
      "\n",
      "MONTESTER:\n",
      "God bless; God bless thee!\n",
      "What is your name, thou art not the gods-\n",
      "That call thee your name, thou art not God\n",
      "\n",
      "[101 | 3996.22] loss=3.04 avg=3.31\n",
      "[102 | 4033.78] loss=3.13 avg=3.31\n",
      "[103 | 4083.56] loss=3.26 avg=3.31\n",
      "[104 | 4119.13] loss=3.18 avg=3.31\n",
      "[105 | 4159.01] loss=3.06 avg=3.30\n",
      "[106 | 4197.04] loss=3.11 avg=3.30\n",
      "[107 | 4234.49] loss=3.17 avg=3.30\n",
      "[108 | 4273.44] loss=3.04 avg=3.30\n",
      "[109 | 4312.10] loss=3.17 avg=3.29\n",
      "[110 | 4351.60] loss=3.28 avg=3.29\n",
      "[111 | 4388.62] loss=3.07 avg=3.29\n",
      "[112 | 4426.04] loss=3.08 avg=3.29\n",
      "[113 | 4465.70] loss=3.09 avg=3.28\n",
      "[114 | 4502.53] loss=2.87 avg=3.28\n",
      "[115 | 4542.50] loss=3.07 avg=3.27\n",
      "[116 | 4580.58] loss=2.88 avg=3.27\n",
      "[117 | 4619.42] loss=3.15 avg=3.27\n",
      "[118 | 4659.57] loss=3.12 avg=3.27\n",
      "[119 | 4696.11] loss=2.87 avg=3.26\n",
      "[120 | 4734.33] loss=3.14 avg=3.26\n",
      "[121 | 4773.57] loss=3.21 avg=3.26\n",
      "[122 | 4811.42] loss=3.27 avg=3.26\n",
      "[123 | 4851.70] loss=3.47 avg=3.26\n",
      "[124 | 4889.59] loss=3.07 avg=3.26\n",
      "[125 | 4927.62] loss=2.86 avg=3.25\n",
      "[126 | 4969.05] loss=3.32 avg=3.25\n",
      "[127 | 5005.21] loss=3.43 avg=3.26\n",
      "[128 | 5045.16] loss=3.24 avg=3.26\n",
      "[129 | 5083.05] loss=3.11 avg=3.25\n",
      "[130 | 5120.77] loss=2.95 avg=3.25\n",
      "[131 | 5160.60] loss=3.23 avg=3.25\n",
      "[132 | 5199.00] loss=3.18 avg=3.25\n",
      "[133 | 5237.89] loss=2.87 avg=3.24\n",
      "[134 | 5285.16] loss=2.81 avg=3.24\n",
      "[135 | 5319.21] loss=3.08 avg=3.23\n",
      "[136 | 5356.85] loss=2.91 avg=3.23\n",
      "[137 | 5395.99] loss=3.01 avg=3.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138 | 5433.90] loss=3.06 avg=3.23\n",
      "[139 | 5472.52] loss=3.17 avg=3.22\n",
      "[140 | 5510.82] loss=2.99 avg=3.22\n",
      "[141 | 5549.31] loss=3.31 avg=3.22\n",
      "[142 | 5587.72] loss=3.47 avg=3.23\n",
      "[143 | 5625.58] loss=3.17 avg=3.23\n",
      "[144 | 5662.52] loss=3.34 avg=3.23\n",
      "[145 | 5701.29] loss=3.02 avg=3.22\n",
      "[146 | 5739.38] loss=3.10 avg=3.22\n",
      "[147 | 5776.87] loss=3.02 avg=3.22\n",
      "[148 | 5815.98] loss=2.99 avg=3.22\n",
      "[149 | 5853.82] loss=3.18 avg=3.22\n",
      "[150 | 5892.64] loss=2.90 avg=3.21\n",
      "[151 | 5930.50] loss=3.07 avg=3.21\n",
      "[152 | 5969.21] loss=2.97 avg=3.21\n",
      "[153 | 6006.75] loss=3.08 avg=3.21\n",
      "[154 | 6045.94] loss=2.99 avg=3.20\n",
      "[155 | 6084.25] loss=2.93 avg=3.20\n",
      "[156 | 6122.71] loss=2.98 avg=3.20\n",
      "[157 | 6160.54] loss=3.00 avg=3.19\n",
      "[158 | 6199.69] loss=2.96 avg=3.19\n",
      "[159 | 6236.69] loss=2.99 avg=3.19\n",
      "[160 | 6275.42] loss=3.06 avg=3.19\n",
      "[161 | 6312.75] loss=3.06 avg=3.19\n",
      "[162 | 6351.52] loss=2.93 avg=3.18\n",
      "[163 | 6389.12] loss=3.26 avg=3.18\n",
      "[164 | 6427.51] loss=3.19 avg=3.18\n",
      "[165 | 6466.28] loss=2.81 avg=3.18\n",
      "[166 | 6505.55] loss=2.93 avg=3.18\n",
      "[167 | 6543.29] loss=2.97 avg=3.17\n",
      "[168 | 6581.34] loss=2.96 avg=3.17\n",
      "[169 | 6620.84] loss=3.04 avg=3.17\n",
      "[170 | 6658.78] loss=3.03 avg=3.17\n",
      "[171 | 6696.67] loss=2.97 avg=3.16\n",
      "[172 | 6734.94] loss=3.07 avg=3.16\n",
      "[173 | 6772.58] loss=3.04 avg=3.16\n",
      "[174 | 6811.75] loss=2.95 avg=3.16\n",
      "[175 | 6849.86] loss=2.99 avg=3.16\n",
      "[176 | 6888.63] loss=3.08 avg=3.16\n",
      "[177 | 6926.64] loss=3.29 avg=3.16\n",
      "[178 | 6965.71] loss=2.77 avg=3.15\n",
      "[179 | 7003.67] loss=3.06 avg=3.15\n",
      "[180 | 7042.39] loss=3.01 avg=3.15\n",
      "[181 | 7080.15] loss=2.86 avg=3.15\n",
      "[182 | 7118.98] loss=2.77 avg=3.14\n",
      "[183 | 7157.08] loss=2.90 avg=3.14\n",
      "[184 | 7195.47] loss=3.12 avg=3.14\n",
      "[185 | 7233.20] loss=3.15 avg=3.14\n",
      "[186 | 7271.90] loss=3.02 avg=3.14\n",
      "[187 | 7309.94] loss=3.18 avg=3.14\n",
      "[188 | 7347.98] loss=2.95 avg=3.14\n",
      "[189 | 7386.25] loss=2.96 avg=3.13\n",
      "[190 | 7424.34] loss=2.90 avg=3.13\n",
      "[191 | 7462.37] loss=3.03 avg=3.13\n",
      "[192 | 7500.92] loss=2.60 avg=3.12\n",
      "[193 | 7538.39] loss=3.01 avg=3.12\n",
      "[194 | 7577.62] loss=2.99 avg=3.12\n",
      "[195 | 7615.21] loss=2.97 avg=3.12\n",
      "[196 | 7653.78] loss=2.83 avg=3.12\n",
      "[197 | 7691.82] loss=3.15 avg=3.12\n",
      "[198 | 7740.87] loss=2.78 avg=3.11\n",
      "[199 | 7786.13] loss=2.79 avg=3.11\n",
      "[200 | 7820.61] loss=3.09 avg=3.11\n",
      "======== SAMPLE 1 ========\n",
      "\n",
      "\n",
      "BRADY:\n",
      "What!\n",
      "\n",
      "JULIET:\n",
      "And how can it be?\n",
      "\n",
      "GLOUCESTER:\n",
      "I know that thou art dead,\n",
      "O my Lord, the Lord of All:\n",
      "A body thy very soul has put in,\n",
      "That is a coffin of thy body,\n",
      "It being that body that dies.\n",
      "\n",
      "GLOUCESTER:\n",
      "I will not believe it, brother;\n",
      "I am sure it is a ghostly dream,\n",
      "Because this flesh's soul die to give;\n",
      "And if ever I be wrong'd, say it me.\n",
      "\n",
      "AUROR:\n",
      "O, brother, tell the truth.\n",
      "\n",
      "JULIET:\n",
      "O thou brother of the great Marcius!\n",
      "What fortune hath made thee out upon,\n",
      "For thus shalt thou die,\n",
      "And from the grave thy body shalt dwell,\n",
      "And hence to heaven thou shalt live:\n",
      "And now, to all the world, I have been.\n",
      "\n",
      "EDWARD:\n",
      "Now, gentle brother, take my hand,\n",
      "And give me the letter, and tell me,\n",
      "That this letter be given me to thy chamber,\n",
      "And my dear brother be told.\n",
      "\n",
      "Lords:\n",
      "Now, I will give thee my brother's name:\n",
      "My son of Lancaster is at court,\n",
      "A kinsman of mine. I pray you, take the letter.\n",
      "\n",
      "HORTENSIO:\n",
      "This is the letter that my liege Marcius got\n",
      "The Earl of Salisbury from Richard the Marshal;\n",
      "And, brother, when thy soul shall have leisure\n",
      "To be anointed by the Lord thy God,\n",
      "The Lord shall deliver it unto thyself.\n",
      "\n",
      "Lords:\n",
      "The letter of Lancaster will not have time to be read,\n",
      "But will be read with pen and paper.\n",
      "\n",
      "HORTENSIO:\n",
      "Now, the king of the world's lords, I will give\n",
      "I' the letter of Lancaster. Now, tell me,\n",
      "Where is the Earl of Salisbury?\n",
      "\n",
      "GLOUCESTER:\n",
      "Bid me the good lord\n",
      "That I should answer your letters.\n",
      "\n",
      "HORTENSIO:\n",
      "He will not know you speak better than I:\n",
      "Therefore, my grace, let him read it.\n",
      "\n",
      "Lords:\n",
      "What is the letter of Lancaster?\n",
      "\n",
      "GLOUCESTER:\n",
      "Say, it says, that the Earl of Salisbury,\n",
      "Who hath given a pardon to many Richard\n",
      "Hath been martyred; or that the duke's son:\n",
      "He will therefore be read with pen and paper.\n",
      "\n",
      "HORTENSIO:\n",
      "And so I hear: if the king's pardon be pard'd,\n",
      "Then the Earl of Salisbury shall live on.\n",
      "\n",
      "Servants:\n",
      "What? what's the man?\n",
      "\n",
      "ESCALUS:\n",
      "I can tell by the book of the duke\n",
      "A man, who was most valiant in battle,\n",
      "Had been slain by the Earl of Salisbury;\n",
      "And he, my lord, gave away to the duke.\n",
      "\n",
      "BRADY:\n",
      "Aurelius, I shall tell you this,\n",
      "Which I had been very much afraid of.\n",
      "\n",
      "MENENIUS:\n",
      "\n",
      "MENENIUS:\n",
      "\n",
      "MENENIUS:\n",
      "\n",
      "MENENIUS:\n",
      "\n",
      "MENENIUS:\n",
      "\n",
      "MENENIUS:\n",
      "\n",
      "MENENIUS:\n",
      "\n",
      "MENENIUS:\n",
      "\n",
      "MENENIUS:\n",
      "Why, let that be so; I will not go along:\n",
      "I have read his faults, my friend and fellow:\n",
      "And the Lord Gloucester himself has, by the grace of God\n",
      "Seeking in a bloody battle for his life,\n",
      "Seek'd out to kill him: 'tis his fortune,\n",
      "Though he kill'd his kinsman.\n",
      "\n",
      "ISABELLA:\n",
      "Well, brother, let us hear from the Earl of Salisbury.\n",
      "\n",
      "SICINIUS:\n",
      "And have we;\n",
      "Who have, brothers, but have we all.\n",
      "\n",
      "MENENIUS:\n",
      "Brother, if you tell my lord, we would wish\n",
      "Your grace to come to you in a more frank spirit.\n",
      "\n",
      "ESCALUS:\n",
      "You'll come with me to him, my lord;\n",
      "He is a Christian, and is most holy;\n",
      "And is sure you will not say 'You are a Christian'?\n",
      "\n",
      "MENENIUS:\n",
      "No, well, good Lord; he seems a duke,\n",
      "And he will do the duke no harm.\n",
      "\n",
      "MENENIUS:\n",
      "Sir, he is a duke, though he seem to be.\n",
      "\n",
      "SICINIUS:\n",
      "No.\n",
      "\n",
      "MENENIUS:\n",
      "But he does not wish to be,\n",
      "Unless it be so: he hath\n",
      "\n",
      "[201 | 7943.59] loss=2.93 avg=3.11\n",
      "[202 | 7980.38] loss=2.87 avg=3.10\n",
      "[203 | 8029.81] loss=2.89 avg=3.10\n",
      "[204 | 8065.79] loss=2.87 avg=3.10\n",
      "[205 | 8104.77] loss=2.79 avg=3.10\n",
      "[206 | 8143.18] loss=3.03 avg=3.09\n",
      "[207 | 8180.60] loss=2.66 avg=3.09\n",
      "[208 | 8220.14] loss=2.69 avg=3.09\n",
      "[209 | 8258.73] loss=2.76 avg=3.08\n",
      "[210 | 8295.46] loss=3.06 avg=3.08\n",
      "[211 | 8333.89] loss=2.79 avg=3.08\n",
      "[212 | 8373.75] loss=3.00 avg=3.08\n",
      "[213 | 8409.66] loss=2.94 avg=3.08\n",
      "[214 | 8449.05] loss=2.78 avg=3.07\n",
      "[215 | 8489.49] loss=2.77 avg=3.07\n",
      "[216 | 8524.51] loss=3.00 avg=3.07\n",
      "[217 | 8565.01] loss=2.82 avg=3.07\n",
      "[218 | 8602.80] loss=2.72 avg=3.06\n",
      "[219 | 8640.08] loss=2.88 avg=3.06\n",
      "[220 | 8680.18] loss=2.89 avg=3.06\n",
      "[221 | 8717.89] loss=2.92 avg=3.06\n",
      "[222 | 8756.08] loss=2.89 avg=3.05\n",
      "[223 | 8796.56] loss=3.09 avg=3.05\n",
      "[224 | 8834.50] loss=2.56 avg=3.05\n",
      "[225 | 8873.76] loss=3.05 avg=3.05\n",
      "[226 | 8911.51] loss=2.78 avg=3.05\n",
      "[227 | 8951.01] loss=2.76 avg=3.04\n",
      "[228 | 8988.41] loss=2.72 avg=3.04\n",
      "[229 | 9027.13] loss=2.82 avg=3.04\n",
      "[230 | 9066.82] loss=2.82 avg=3.03\n",
      "[231 | 9104.45] loss=2.64 avg=3.03\n",
      "[232 | 9141.01] loss=2.79 avg=3.03\n",
      "[233 | 9182.55] loss=2.95 avg=3.03\n",
      "[234 | 9218.87] loss=3.10 avg=3.03\n",
      "[235 | 9255.32] loss=2.77 avg=3.02\n",
      "[236 | 9290.34] loss=2.92 avg=3.02\n",
      "[237 | 9324.86] loss=2.53 avg=3.02\n",
      "[238 | 9360.71] loss=2.71 avg=3.01\n",
      "[239 | 9394.26] loss=2.76 avg=3.01\n",
      "[240 | 9429.77] loss=2.97 avg=3.01\n",
      "[241 | 9465.76] loss=3.07 avg=3.01\n",
      "[242 | 9498.96] loss=2.81 avg=3.01\n",
      "[243 | 9535.41] loss=2.98 avg=3.01\n",
      "[244 | 9571.44] loss=2.69 avg=3.01\n",
      "[245 | 9604.07] loss=2.67 avg=3.00\n",
      "[246 | 9641.65] loss=2.79 avg=3.00\n",
      "[247 | 9677.11] loss=2.96 avg=3.00\n",
      "[248 | 9710.72] loss=2.68 avg=3.00\n",
      "[249 | 9746.34] loss=2.82 avg=2.99\n",
      "[250 | 9782.83] loss=2.76 avg=2.99\n",
      "[251 | 9815.35] loss=2.77 avg=2.99\n",
      "[252 | 9850.58] loss=2.89 avg=2.99\n",
      "[253 | 9886.23] loss=2.98 avg=2.99\n",
      "[254 | 9921.40] loss=2.61 avg=2.98\n",
      "[255 | 9955.85] loss=2.69 avg=2.98\n",
      "[256 | 9989.56] loss=2.62 avg=2.98\n",
      "[257 | 10026.91] loss=3.11 avg=2.98\n",
      "[258 | 10060.01] loss=2.83 avg=2.98\n",
      "[259 | 10093.89] loss=2.91 avg=2.98\n",
      "[260 | 10129.81] loss=2.89 avg=2.97\n",
      "[261 | 10164.89] loss=2.84 avg=2.97\n",
      "[262 | 10199.32] loss=2.58 avg=2.97\n",
      "[263 | 10234.14] loss=2.50 avg=2.96\n",
      "[264 | 10270.74] loss=2.65 avg=2.96\n",
      "[265 | 10303.52] loss=2.65 avg=2.96\n",
      "[266 | 10337.09] loss=2.86 avg=2.96\n",
      "[267 | 10373.06] loss=2.73 avg=2.95\n",
      "[268 | 10407.34] loss=2.40 avg=2.95\n",
      "[269 | 10440.74] loss=2.71 avg=2.95\n",
      "[270 | 10474.60] loss=2.58 avg=2.94\n",
      "[271 | 10510.73] loss=2.91 avg=2.94\n",
      "[272 | 10543.67] loss=2.56 avg=2.94\n",
      "[273 | 10578.96] loss=2.71 avg=2.93\n",
      "[274 | 10614.34] loss=2.80 avg=2.93\n",
      "[275 | 10649.48] loss=2.68 avg=2.93\n",
      "[276 | 10682.82] loss=2.65 avg=2.93\n",
      "[277 | 10716.59] loss=2.44 avg=2.92\n",
      "[278 | 10753.75] loss=2.79 avg=2.92\n",
      "[279 | 10786.57] loss=2.60 avg=2.92\n",
      "[280 | 10821.45] loss=2.53 avg=2.91\n",
      "[281 | 10855.67] loss=2.64 avg=2.91\n",
      "[282 | 10890.76] loss=2.91 avg=2.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[283 | 10924.84] loss=2.81 avg=2.91\n",
      "[284 | 10957.19] loss=2.75 avg=2.91\n",
      "[285 | 10995.30] loss=2.63 avg=2.90\n",
      "[286 | 11027.35] loss=2.81 avg=2.90\n",
      "[287 | 11062.54] loss=2.64 avg=2.90\n",
      "[288 | 11098.43] loss=2.61 avg=2.90\n",
      "[289 | 11131.85] loss=2.70 avg=2.90\n",
      "[290 | 11168.64] loss=2.71 avg=2.89\n",
      "[291 | 11202.20] loss=2.54 avg=2.89\n",
      "[292 | 11239.80] loss=2.59 avg=2.89\n",
      "[293 | 11274.12] loss=2.90 avg=2.89\n",
      "[294 | 11308.97] loss=2.27 avg=2.88\n",
      "[295 | 11346.10] loss=2.59 avg=2.88\n",
      "[296 | 11380.08] loss=2.63 avg=2.87\n",
      "[297 | 11414.05] loss=2.77 avg=2.87\n",
      "[298 | 11447.67] loss=2.43 avg=2.87\n",
      "[299 | 11483.47] loss=2.58 avg=2.87\n",
      "[300 | 11516.54] loss=2.76 avg=2.86\n",
      "======== SAMPLE 1 ========\n",
      "BRINIUS:\n",
      "Why, 'tis known.\n",
      "\n",
      "RATCLIFF:\n",
      "'Twas a traitor's name;\n",
      "And, being an unpardon'd traitor,\n",
      "Yet some innocent blood is shed in him;\n",
      "And, 'tis known; and 'twas yet a man\n",
      "His father and his brother both made an honest\n",
      "Saw of this offence, and, he said, there were\n",
      "About him, some of the greatest number,\n",
      "Which were the sons of Coriolanus;\n",
      "And there were in him that were the daughters;\n",
      "And, most of all, a great number,\n",
      "Of all great authority.\n",
      "\n",
      "RATCLIFF:\n",
      "They had; indeed: here is that body where my father died.\n",
      "\n",
      "DION:\n",
      "He is a prophet of good news, and a good soldier:\n",
      "He hath not been slain for treason:\n",
      "He died not to serve the gods, nor for the devil's sake,\n",
      "But to pay homage to the good gods.\n",
      "\n",
      "RATCLIFF:\n",
      "I thank thee, good shepherd.\n",
      "\n",
      "DION:\n",
      "To be revenged on me, but to serve the gods.\n",
      "\n",
      "HORTENSIO:\n",
      "Let them be confederate: let them be slaves; for that act\n",
      "Should have made thee so proud. Let them be made true;\n",
      "For if they have any blood in them, let them do it.\n",
      "\n",
      "HORTENSIO:\n",
      "How can so be a life to you?\n",
      "\n",
      "DION:\n",
      "To that end.\n",
      "\n",
      "HORTENSIO:\n",
      "What, I say? What counsel can you tell me?\n",
      "\n",
      "DION:\n",
      "Well, no more than what the gods shall command me.\n",
      "\n",
      "HORTENSIO:\n",
      "But what I shall be, or the contrary shall I:\n",
      "I must, then, by the faith of my conscience,\n",
      "Doubt it not: the gods put my life into mine;\n",
      "But in all honesty, I have trusty counsels.\n",
      "\n",
      "DION:\n",
      "Do you know it not? If I say so, I must for my life\n",
      "Be put to death; for they will believe it not.\n",
      "\n",
      "HORTENSIO:\n",
      "Then, by my life I swear, if there were no life,\n",
      "I would betide them the gods.\n",
      "\n",
      "DION:\n",
      "But to kill a creature\n",
      "I cannot stand any.\n",
      "\n",
      "HORTENSIO:\n",
      "Let them be slain for being false: they will not live.\n",
      "\n",
      "DION:\n",
      "But you can.\n",
      "\n",
      "HORTENSIO:\n",
      "My oath I'll prove\n",
      "To me by blood, and not by vow;\n",
      "I will die for the life I have lost.\n",
      "\n",
      "DION:\n",
      "I will, then, to put you to death: else I would die;\n",
      "And I do swear you, not by this, nor that oath.\n",
      "That oath will prove me no match.\n",
      "Take thou this man, that you take up the sword.\n",
      "\n",
      "HORTENSIO:\n",
      "I'll prove him no match; he cannot be my father:\n",
      "I have made him my deceiver, and will prove all false.\n",
      "He is an unpair'd traitor: put him to death;\n",
      "For once I saw him kill a living man,\n",
      "And here comes the murderer; no man can stand him alive;\n",
      "And yet, O upright, faithful, and innocent state\n",
      "That man, I'll swear, I will not be parted.\n",
      "\n",
      "DION:\n",
      "What, no? let him be strangled to death;\n",
      "If this sentence any thing should stay his life,\n",
      "I'll make him a martyr to my own blood.\n",
      "\n",
      "HORTENSIO:\n",
      "But his head be thrown to the fire, his body burn\n",
      "For that he was not born to die: there is nothing in me\n",
      "But a bloody oath: for that I have no blood to shed\n",
      "Now in the life of these my enemies\n",
      "What I did for them I have shed.\n",
      "\n",
      "DION:\n",
      "Well, take, and by your life or suffer life or death\n",
      "By him to death, for this he is not your father's:\n",
      "The gods put me to death; if I say so,\n",
      "He will take my life upon my life for his life;\n",
      "For both are lawful words to him,\n",
      "Though all men hate to speak to one.\n",
      "\n",
      "HORTENSIO:\n",
      "Away with the man!\n",
      "Away with the man!\n",
      "\n",
      "DION:\n",
      "'Tis now your turn, and you shall know them here.\n",
      "\n",
      "CURTIS:\n",
      "I should be the most tedious of men\n",
      "Even of the most tedious, or else an idle man:\n",
      "Your highness in honesty hath a strange ambition,\n",
      "Which should put you to death, for giving your life:\n",
      "Yet I say, if he should die for that,\n",
      "\n",
      "[301 | 11621.44] loss=2.61 avg=2.86\n",
      "[302 | 11653.15] loss=2.69 avg=2.86\n",
      "[303 | 11691.84] loss=2.89 avg=2.86\n",
      "[304 | 11727.94] loss=2.90 avg=2.86\n",
      "[305 | 11760.95] loss=2.66 avg=2.86\n",
      "[306 | 11795.11] loss=2.84 avg=2.86\n",
      "[307 | 11829.48] loss=2.72 avg=2.86\n",
      "[308 | 11862.73] loss=2.22 avg=2.85\n",
      "[309 | 11898.27] loss=2.80 avg=2.85\n",
      "[310 | 11931.06] loss=2.20 avg=2.84\n",
      "[311 | 11966.86] loss=2.53 avg=2.84\n",
      "[312 | 11999.23] loss=2.75 avg=2.84\n",
      "[313 | 12033.85] loss=2.62 avg=2.84\n",
      "[314 | 12070.60] loss=2.71 avg=2.84\n",
      "[315 | 12103.29] loss=2.44 avg=2.83\n",
      "[316 | 12138.44] loss=2.85 avg=2.83\n",
      "[317 | 12171.63] loss=2.44 avg=2.83\n",
      "[318 | 12206.64] loss=2.62 avg=2.83\n",
      "[319 | 12239.84] loss=2.44 avg=2.82\n",
      "[320 | 12274.91] loss=2.45 avg=2.82\n",
      "[321 | 12310.80] loss=2.25 avg=2.81\n",
      "[322 | 12343.47] loss=2.48 avg=2.81\n",
      "[323 | 12378.72] loss=2.48 avg=2.81\n",
      "[324 | 12412.69] loss=2.22 avg=2.80\n",
      "[325 | 12448.75] loss=2.67 avg=2.80\n",
      "[326 | 12482.06] loss=2.42 avg=2.79\n",
      "[327 | 12517.14] loss=2.78 avg=2.79\n",
      "[328 | 12552.03] loss=2.47 avg=2.79\n",
      "[329 | 12585.68] loss=2.48 avg=2.79\n",
      "[330 | 12621.54] loss=2.48 avg=2.78\n",
      "[331 | 12655.22] loss=2.52 avg=2.78\n",
      "[332 | 12691.11] loss=2.34 avg=2.78\n",
      "[333 | 12723.77] loss=2.75 avg=2.78\n",
      "[334 | 12758.62] loss=2.27 avg=2.77\n",
      "[335 | 12794.23] loss=2.39 avg=2.77\n",
      "[336 | 12828.54] loss=2.51 avg=2.76\n",
      "[337 | 12862.83] loss=2.77 avg=2.76\n",
      "[338 | 12897.25] loss=2.41 avg=2.76\n",
      "[339 | 12931.77] loss=2.49 avg=2.76\n",
      "[340 | 12964.93] loss=2.19 avg=2.75\n",
      "[341 | 13000.54] loss=2.42 avg=2.75\n",
      "[342 | 13034.62] loss=2.63 avg=2.75\n",
      "[343 | 13068.87] loss=2.36 avg=2.74\n",
      "[344 | 13102.35] loss=2.39 avg=2.74\n",
      "[345 | 13135.49] loss=2.18 avg=2.73\n",
      "[346 | 13171.81] loss=2.77 avg=2.73\n",
      "[347 | 13204.34] loss=2.42 avg=2.73\n",
      "[348 | 13239.50] loss=2.45 avg=2.73\n",
      "[349 | 13275.02] loss=2.81 avg=2.73\n",
      "[350 | 13309.82] loss=2.05 avg=2.72\n",
      "[351 | 13343.04] loss=2.64 avg=2.72\n",
      "[352 | 13376.88] loss=2.73 avg=2.72\n",
      "[353 | 13412.38] loss=2.44 avg=2.72\n",
      "[354 | 13446.26] loss=2.45 avg=2.72\n",
      "[355 | 13480.40] loss=2.30 avg=2.71\n",
      "[356 | 13515.29] loss=2.35 avg=2.71\n",
      "[357 | 13549.39] loss=2.42 avg=2.70\n",
      "[358 | 13583.40] loss=2.49 avg=2.70\n",
      "[359 | 13619.20] loss=2.63 avg=2.70\n",
      "[360 | 13654.46] loss=2.85 avg=2.70\n",
      "[361 | 13686.29] loss=2.30 avg=2.70\n",
      "[362 | 13721.44] loss=2.62 avg=2.70\n",
      "[363 | 13756.24] loss=2.55 avg=2.70\n",
      "[364 | 13791.37] loss=2.28 avg=2.69\n",
      "[365 | 13824.96] loss=2.77 avg=2.69\n",
      "[366 | 13860.58] loss=2.55 avg=2.69\n",
      "[367 | 13893.91] loss=2.30 avg=2.69\n",
      "[368 | 13926.13] loss=2.34 avg=2.68\n",
      "[369 | 13962.22] loss=2.26 avg=2.68\n",
      "[370 | 13996.77] loss=2.53 avg=2.68\n",
      "[371 | 14030.03] loss=2.62 avg=2.68\n",
      "[372 | 14063.23] loss=2.34 avg=2.67\n",
      "[373 | 14097.20] loss=2.56 avg=2.67\n",
      "[374 | 14132.47] loss=2.19 avg=2.67\n",
      "[375 | 14165.36] loss=2.03 avg=2.66\n",
      "[376 | 14200.15] loss=2.48 avg=2.66\n",
      "[377 | 14235.08] loss=2.53 avg=2.66\n",
      "[378 | 14268.90] loss=2.43 avg=2.66\n",
      "[379 | 14301.34] loss=2.47 avg=2.65\n",
      "[380 | 14337.91] loss=1.88 avg=2.65\n",
      "[381 | 14370.73] loss=2.33 avg=2.64\n",
      "[382 | 14405.63] loss=2.10 avg=2.64\n",
      "[383 | 14441.11] loss=2.31 avg=2.63\n",
      "[384 | 14473.84] loss=2.23 avg=2.63\n",
      "[385 | 14507.99] loss=2.48 avg=2.63\n",
      "[386 | 14541.00] loss=2.43 avg=2.63\n",
      "[387 | 14576.22] loss=2.47 avg=2.62\n",
      "[388 | 14609.94] loss=2.46 avg=2.62\n",
      "[389 | 14643.28] loss=2.09 avg=2.62\n",
      "[390 | 14677.79] loss=2.21 avg=2.61\n",
      "[391 | 14713.97] loss=2.39 avg=2.61\n",
      "[392 | 14746.35] loss=2.26 avg=2.61\n",
      "[393 | 14781.65] loss=2.32 avg=2.60\n",
      "[394 | 14816.82] loss=2.22 avg=2.60\n",
      "[395 | 14850.17] loss=2.35 avg=2.60\n",
      "[396 | 14885.44] loss=2.24 avg=2.59\n",
      "[397 | 14918.77] loss=2.51 avg=2.59\n",
      "[398 | 14955.06] loss=2.47 avg=2.59\n",
      "[399 | 14988.30] loss=2.29 avg=2.59\n",
      "[400 | 15021.34] loss=2.20 avg=2.59\n",
      "======== SAMPLE 1 ========\n",
      "\n",
      "We are a mighty power, but\n",
      "It is faint to hear;\n",
      "For with our valiant hearts--which are--and\n",
      "Our power weak; and, in despair,\n",
      "Stumbling on one we are tired upon.\n",
      "But, if this be the way you must go,\n",
      "Come along; for you shall go on;\n",
      "But, if you dare not,\n",
      "The lordship is coming to your rescue\n",
      "And we shall have none for your queen.\n",
      "\n",
      "JULIET:\n",
      "O, be certain! my brave brother,\n",
      "Heavens and honour are in me joint\n",
      "Might I had been in arms before your brother.\n",
      "Let's go to the Lord Mayor;\n",
      "And all things, O, be to him as is right!\n",
      "This young and brave fellow is your queen,\n",
      "For whom, in the presence of so many princes,\n",
      "Myself am most thankful; for I have,\n",
      "In that my young self, by his grace and\n",
      "by his counsel, saved thee well.\n",
      "And, now this new-made man approaches,\n",
      "'Tis time to make our joint venture,\n",
      "And if he be brave as he says,\n",
      "We cannot be long behind now.\n",
      "\n",
      "RICHARD:\n",
      "My sovereign, my prince! Why have you been so careful?\n",
      "Now shall you have leave to rejoice at it:\n",
      "To me the joys of happiness came;\n",
      "To you what yours are no joy; and to me,\n",
      "You must rejoice at the loss of my queen.\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "My lord, you have been most attentive.\n",
      "\n",
      "DUKE OF YORK:\n",
      "How far is it that we are in safety?\n",
      "\n",
      "DUKE OF LAURENCE:\n",
      "The walls of safety shall stand in safety,\n",
      "And not the princelings there fear us;\n",
      "Even so, though the wall are great and strong.\n",
      "\n",
      "DUKE OF YORK:\n",
      "I thank my lord and sovereign: it seems he\n",
      "Is not at all ignorant that your son,\n",
      "Of whom he is not well known, is heir,\n",
      "Of which he was not proud: he himself is heir,\n",
      "That by the alliance of several sons\n",
      "He may have a great and a small stake in succession.\n",
      "Your royal personates are but of few,\n",
      "With all the people trembling in dread.\n",
      "\n",
      "DUKE OF YORK:\n",
      "I thank my lord and sovereign: they have done\n",
      "To make the whole world tremble under your feet,\n",
      "And be seen to weep for your sad loss:\n",
      "They have made good news to all, though their friends\n",
      "Do nothing for their comfort: look, this is my son,\n",
      "Who lives by the sword to make the world tremble.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Go, speak with Bolingbroke.\n",
      "\n",
      "BOLINGBROKE:\n",
      "My lord, you are the one most sorrowful:\n",
      "He that loved your queen shall be your jolly rest,\n",
      "And he that did not love your queen shall be mad.\n",
      "\n",
      "DUKE OF YORK:\n",
      "You are all of one complexion: you are\n",
      "Not like to be that pale as this black and white!\n",
      "Than to bear the common eye of Bolingbroke,\n",
      "With whom you love so well: you are pale with frost\n",
      "And would melt your ice-cold ice-cold love.\n",
      "\n",
      "DUKE OF YORK:\n",
      "I know your rage: yet bear your face; your head, head\n",
      "Of all your hateings before him since his wife,\n",
      "For lack of a cause; you are pale,\n",
      "Being one of the many that I have hated:\n",
      "Yet know, even you yourself, I know the cause\n",
      "Which way you have made to move me: my tongue,\n",
      "Which I mean for your benefit, I have neglected:\n",
      "Your brother must be the king that is not king;\n",
      "The Duke of York must be the king that is,\n",
      "You, my wife,--if you will--make haste,\n",
      "And walk like kings before your brother comes;\n",
      "Before he marries, and you the queen dead;\n",
      "Before your queen dies, and Bolingbroke's dead;\n",
      "Before your brother comes in, and King Richard\n",
      "Dies, and the gates ope open,--whither?--\n",
      "Where is your brother? and what is he doing\n",
      "That calls? What helps'st thou, if you say that?\n",
      "\n",
      "DUKE OF YORK:\n",
      "I have been busy: what is his body's doing,\n",
      "How many days, what days, what hours,\n",
      "If any, do I watch on his mind?\n",
      "\n",
      "DUKE OF YORK:\n",
      "Many days, many hours, and, to my weary soul,\n",
      "Loves not so much, as it seems most ends.\n",
      "Ah, brother, a quick hearted brother\n",
      "Comes quick in every encounter with\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[401 | 15123.14] loss=1.89 avg=2.58\n",
      "[402 | 15154.00] loss=2.08 avg=2.57\n",
      "[403 | 15197.03] loss=2.33 avg=2.57\n",
      "[404 | 15231.72] loss=2.44 avg=2.57\n",
      "[405 | 15265.57] loss=2.13 avg=2.56\n",
      "[406 | 15299.24] loss=2.03 avg=2.56\n",
      "[407 | 15333.28] loss=1.74 avg=2.55\n",
      "[408 | 15367.34] loss=2.08 avg=2.55\n",
      "[409 | 15400.07] loss=2.20 avg=2.54\n",
      "[410 | 15434.30] loss=2.07 avg=2.54\n",
      "[411 | 15467.75] loss=2.06 avg=2.53\n",
      "[412 | 15501.82] loss=2.07 avg=2.53\n",
      "[413 | 15535.91] loss=1.90 avg=2.52\n",
      "[414 | 15570.50] loss=2.57 avg=2.52\n",
      "[415 | 15603.51] loss=2.21 avg=2.52\n",
      "[416 | 15636.23] loss=2.43 avg=2.52\n",
      "[417 | 15673.09] loss=2.03 avg=2.51\n",
      "[418 | 15705.15] loss=2.32 avg=2.51\n",
      "[419 | 15738.34] loss=1.95 avg=2.51\n",
      "[420 | 15772.97] loss=2.40 avg=2.50\n",
      "[421 | 15806.42] loss=2.35 avg=2.50\n",
      "[422 | 15840.39] loss=2.20 avg=2.50\n",
      "[423 | 15874.51] loss=1.81 avg=2.49\n",
      "[424 | 15910.36] loss=2.21 avg=2.49\n",
      "[425 | 15943.41] loss=2.11 avg=2.49\n",
      "[426 | 15978.57] loss=2.20 avg=2.48\n",
      "[427 | 16011.97] loss=1.98 avg=2.48\n",
      "[428 | 16047.72] loss=2.17 avg=2.48\n",
      "[429 | 16082.49] loss=2.21 avg=2.47\n",
      "[430 | 16115.56] loss=2.06 avg=2.47\n",
      "[431 | 16151.15] loss=1.73 avg=2.46\n",
      "[432 | 16183.98] loss=2.15 avg=2.46\n",
      "[433 | 16217.32] loss=2.23 avg=2.46\n",
      "[434 | 16251.79] loss=2.53 avg=2.46\n",
      "[435 | 16287.27] loss=2.09 avg=2.45\n",
      "[436 | 16321.27] loss=1.82 avg=2.45\n",
      "[437 | 16354.53] loss=2.35 avg=2.45\n",
      "[438 | 16390.14] loss=2.08 avg=2.44\n",
      "[439 | 16422.22] loss=1.92 avg=2.44\n",
      "[440 | 16456.46] loss=1.95 avg=2.43\n",
      "[441 | 16489.37] loss=2.22 avg=2.43\n",
      "[442 | 16524.19] loss=2.44 avg=2.43\n",
      "[443 | 16556.98] loss=1.98 avg=2.42\n",
      "[444 | 16592.73] loss=2.21 avg=2.42\n",
      "[445 | 16626.97] loss=2.14 avg=2.42\n",
      "[446 | 16661.34] loss=1.59 avg=2.41\n",
      "[447 | 16694.95] loss=2.28 avg=2.41\n",
      "[448 | 16728.68] loss=1.97 avg=2.41\n",
      "[449 | 16763.96] loss=2.00 avg=2.40\n",
      "[450 | 16796.25] loss=1.71 avg=2.39\n",
      "[451 | 16832.29] loss=2.05 avg=2.39\n",
      "[452 | 16865.40] loss=2.22 avg=2.39\n",
      "[453 | 16900.63] loss=1.85 avg=2.38\n",
      "[454 | 16935.01] loss=2.14 avg=2.38\n",
      "[455 | 16966.94] loss=2.02 avg=2.38\n",
      "[456 | 17003.87] loss=2.12 avg=2.37\n",
      "[457 | 17036.48] loss=1.75 avg=2.37\n",
      "[458 | 17069.88] loss=2.60 avg=2.37\n",
      "[459 | 17106.13] loss=2.06 avg=2.37\n",
      "[460 | 17139.94] loss=1.96 avg=2.36\n",
      "[461 | 17172.80] loss=1.89 avg=2.36\n",
      "[462 | 17208.83] loss=1.88 avg=2.35\n",
      "[463 | 17243.19] loss=2.26 avg=2.35\n",
      "[464 | 17275.23] loss=2.04 avg=2.35\n",
      "[465 | 17311.57] loss=2.33 avg=2.35\n",
      "[466 | 17345.13] loss=1.88 avg=2.34\n",
      "[467 | 17380.91] loss=1.85 avg=2.34\n",
      "[468 | 17414.82] loss=1.51 avg=2.33\n",
      "[469 | 17448.29] loss=1.76 avg=2.33\n",
      "[470 | 17483.07] loss=2.27 avg=2.33\n",
      "[471 | 17515.74] loss=2.00 avg=2.32\n",
      "[472 | 17550.69] loss=1.90 avg=2.32\n",
      "[473 | 17585.04] loss=1.83 avg=2.31\n",
      "[474 | 17619.53] loss=2.36 avg=2.31\n",
      "[475 | 17652.77] loss=1.85 avg=2.31\n",
      "[476 | 17688.03] loss=2.25 avg=2.31\n",
      "[477 | 17721.80] loss=1.64 avg=2.30\n",
      "[478 | 17755.66] loss=1.67 avg=2.29\n",
      "[479 | 17791.48] loss=1.96 avg=2.29\n",
      "[480 | 17824.52] loss=1.98 avg=2.29\n",
      "[481 | 17859.75] loss=2.13 avg=2.29\n",
      "[482 | 17892.25] loss=2.27 avg=2.29\n",
      "[483 | 17927.63] loss=2.11 avg=2.28\n",
      "[484 | 17962.14] loss=2.03 avg=2.28\n",
      "[485 | 17994.47] loss=1.78 avg=2.28\n",
      "[486 | 18029.97] loss=2.13 avg=2.28\n",
      "[487 | 18063.40] loss=2.09 avg=2.27\n",
      "[488 | 18098.71] loss=1.84 avg=2.27\n",
      "[489 | 18131.27] loss=1.74 avg=2.26\n",
      "[490 | 18166.14] loss=1.82 avg=2.26\n",
      "[491 | 18201.73] loss=1.83 avg=2.26\n",
      "[492 | 18233.68] loss=1.64 avg=2.25\n",
      "[493 | 18270.41] loss=2.51 avg=2.25\n",
      "[494 | 18304.42] loss=2.34 avg=2.25\n",
      "[495 | 18338.21] loss=2.05 avg=2.25\n",
      "[496 | 18371.83] loss=1.93 avg=2.25\n",
      "[497 | 18408.19] loss=2.42 avg=2.25\n",
      "[498 | 18442.61] loss=2.30 avg=2.25\n",
      "[499 | 18475.98] loss=2.00 avg=2.25\n",
      "[500 | 18510.41] loss=1.72 avg=2.24\n",
      "======== SAMPLE 1 ========\n",
      "ous to thy bosom.\n",
      "The sun hath some light in his eye,\n",
      "But none bright in his eye,\n",
      "Like some great moon or a rich sun.\n",
      "But come, get thee gone.\n",
      "\n",
      "CORIOLANUS:\n",
      "Nay, bethink you, since thou art so swift,\n",
      "It will not hurt that I like thee most.\n",
      "\n",
      "ANTONIO:\n",
      "I know not what to say.\n",
      "\n",
      "CORIOLANUS:\n",
      "My best fears are, that I am of better birth\n",
      "Than they are of any kindred;\n",
      "And they are not so galled, as I am,\n",
      "To say, I am a baby.\n",
      "\n",
      "VIRGILIA:\n",
      "But, sir,\n",
      "They shall answer to such things as we have.\n",
      "\n",
      "CORIOLANUS:\n",
      "Come, come, we are well galled by the gods:\n",
      "No more harm may come upon our backs,\n",
      "For if your hearts were against it, you were slaves.\n",
      "\n",
      "VIRGILIA:\n",
      "No, sir.\n",
      "\n",
      "CORIOLANUS:\n",
      "Let's go.\n",
      "Thou'rt some light in thy eye:\n",
      "It shall be my fault if thy looks do loll.\n",
      "I'll fain call thee by any name,\n",
      "And never by any name nor type.\n",
      "\n",
      "VIRGILIA:\n",
      "Why, 'light' never a name!\n",
      "It is an ancient female name.\n",
      "\n",
      "CORIOLANUS:\n",
      "But 'light' we will use until our times\n",
      "Have made eyes of 'spectacles.'\n",
      "\n",
      "VIRGILIA:\n",
      "Why, 'spectacles,' wilt thou call a thing light?\n",
      "\n",
      "CORIOLANUS:\n",
      "Well, 'spectacles,' wilt thou call me by any name?\n",
      "\n",
      "VIRGILIA:\n",
      "Well, well, well; or thou dost fool me.\n",
      "The sun is an old female name;\n",
      "And 'light is' never the lightest name.\n",
      "\n",
      "CORIOLANUS:\n",
      "Good madam, say not thou that name is fowl.\n",
      "The wisp of her eye, for one name,\n",
      "Should fly from an egg and run o'er\n",
      "To a wife and her children o' the nest;\n",
      "For 'spectacles' never so much\n",
      "Inflate an egg as mark it out.\n",
      "\n",
      "VIRGILIA:\n",
      "Grumio, take me by the hand\n",
      "And by my poor finger: my name's been seen\n",
      "Too many times to name a name, but 'spectacles\n",
      "Thou shalt:' the fault of my ignorance,\n",
      "I mean you, my mistress.\n",
      "\n",
      "CORIOLANUS:\n",
      "Hush, indeed, hush, hush!\n",
      "The matter is thus. I shall be your wife:\n",
      "I'll give you leave to go to Corioli,\n",
      "Of whom I can tell you one thing:\n",
      "You have seen, and felt, and thought, and thought,\n",
      "A strange creature to be that way or the other:\n",
      "It hath made you tremble.\n",
      "If I may entreat you something,\n",
      "My favour: I shall entreat it.\n",
      "\n",
      "First Senator:\n",
      "Good madam,\n",
      "Fear not, it stirs in you. You are but a maid;\n",
      "And you shall better hope that I will not scold you.\n",
      "\n",
      "VIRGILIA:\n",
      "My fair lord!\n",
      "\n",
      "CORIOLANUS:\n",
      "Good madam,\n",
      "Do what you will with all the patience\n",
      "That I have here. But, madam,\n",
      "My sister Coriolanus is still living,\n",
      "And in Corioli has more body-shamed than\n",
      "The spirits of all the rest.\n",
      "\n",
      "VIRGILIA:\n",
      "O, sir!\n",
      "\n",
      "CORIOLANUS:\n",
      "What, weeping? look thou on the world with\n",
      "A murmur that sounds like thunder: I would a mother\n",
      "Had given me two maids to take care of.\n",
      "\n",
      "VIRGILIA:\n",
      "O my child, my goddess!\n",
      "\n",
      "CORIOLANUS:\n",
      "What would a mother do with two maid\n",
      "maids\n",
      "if they did not go with thee?\n",
      "\n",
      "VIRGILIA:\n",
      "I would a mother do a mother-like\n",
      "forgetting of her child.\n",
      "\n",
      "CORIOLANUS:\n",
      "A mother, I beseech you!\n",
      "How chance the babe did not hear you?\n",
      "\n",
      "VIRGILIA:\n",
      "I would she had: O, she was not a maid,\n",
      "But one that have bear'd her; nor was she any thing;\n",
      "For she had none of the braggart that I\n",
      "Have known to have borne.\n",
      "\n",
      "CORIOLANUS:\n",
      "She had none of me;\n",
      "But one for whom you have cursed the\n",
      "m\n",
      "\n",
      "[501 | 18613.40] loss=2.04 avg=2.24\n",
      "[502 | 18643.16] loss=1.94 avg=2.24\n",
      "[503 | 18687.99] loss=1.59 avg=2.23\n",
      "[504 | 18722.65] loss=1.79 avg=2.23\n",
      "[505 | 18756.45] loss=2.07 avg=2.22\n",
      "[506 | 18790.94] loss=1.77 avg=2.22\n",
      "[507 | 18824.29] loss=1.89 avg=2.22\n",
      "[508 | 18857.98] loss=1.67 avg=2.21\n",
      "[509 | 18891.65] loss=2.31 avg=2.21\n",
      "[510 | 18926.40] loss=2.22 avg=2.21\n",
      "[511 | 18959.66] loss=1.76 avg=2.21\n",
      "[512 | 18993.98] loss=1.67 avg=2.20\n",
      "[513 | 19028.28] loss=2.01 avg=2.20\n",
      "[514 | 19070.92] loss=1.46 avg=2.19\n",
      "[515 | 19105.63] loss=1.80 avg=2.19\n",
      "[516 | 19136.37] loss=1.34 avg=2.18\n",
      "[517 | 19171.47] loss=1.85 avg=2.18\n",
      "[518 | 19205.55] loss=1.79 avg=2.17\n",
      "[519 | 19239.22] loss=2.10 avg=2.17\n",
      "[520 | 19274.61] loss=1.81 avg=2.17\n",
      "[521 | 19308.62] loss=1.87 avg=2.17\n",
      "[522 | 19341.42] loss=1.63 avg=2.16\n",
      "[523 | 19383.39] loss=2.06 avg=2.16\n",
      "[524 | 19419.40] loss=1.51 avg=2.15\n",
      "[525 | 19449.38] loss=1.66 avg=2.15\n",
      "[526 | 19485.28] loss=2.29 avg=2.15\n",
      "[527 | 19519.87] loss=1.79 avg=2.15\n",
      "[528 | 19553.34] loss=1.99 avg=2.14\n",
      "[529 | 19586.94] loss=1.92 avg=2.14\n",
      "[530 | 19623.41] loss=1.90 avg=2.14\n",
      "[531 | 19656.78] loss=1.78 avg=2.14\n",
      "[532 | 19690.15] loss=1.83 avg=2.13\n",
      "[533 | 19724.99] loss=1.72 avg=2.13\n",
      "[534 | 19758.92] loss=1.92 avg=2.13\n",
      "[535 | 19791.94] loss=1.64 avg=2.12\n",
      "[536 | 19825.53] loss=2.01 avg=2.12\n",
      "[537 | 19860.07] loss=1.78 avg=2.12\n",
      "[538 | 19894.78] loss=1.82 avg=2.11\n",
      "[539 | 19927.96] loss=1.78 avg=2.11\n",
      "[540 | 19962.05] loss=1.79 avg=2.11\n",
      "[541 | 19997.44] loss=1.68 avg=2.10\n",
      "[542 | 20030.33] loss=1.58 avg=2.10\n",
      "[543 | 20065.63] loss=1.93 avg=2.10\n",
      "[544 | 20101.55] loss=1.57 avg=2.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[545 | 20133.68] loss=1.40 avg=2.08\n",
      "[546 | 20168.98] loss=1.58 avg=2.08\n",
      "[547 | 20202.91] loss=1.60 avg=2.07\n",
      "[548 | 20237.68] loss=1.95 avg=2.07\n",
      "[549 | 20271.70] loss=1.50 avg=2.07\n",
      "[550 | 20305.80] loss=1.87 avg=2.07\n",
      "[551 | 20340.05] loss=1.65 avg=2.06\n",
      "[552 | 20374.62] loss=1.36 avg=2.05\n",
      "[553 | 20409.04] loss=1.82 avg=2.05\n",
      "[554 | 20443.94] loss=1.64 avg=2.05\n",
      "[555 | 20478.75] loss=2.18 avg=2.05\n",
      "[556 | 20511.04] loss=1.30 avg=2.04\n",
      "[557 | 20545.67] loss=1.90 avg=2.04\n",
      "[558 | 20580.56] loss=2.39 avg=2.04\n",
      "[559 | 20614.59] loss=1.84 avg=2.04\n",
      "[560 | 20648.35] loss=1.96 avg=2.04\n",
      "[561 | 20681.38] loss=1.39 avg=2.03\n",
      "[562 | 20717.96] loss=1.72 avg=2.03\n",
      "[563 | 20751.06] loss=1.49 avg=2.03\n",
      "[564 | 20787.37] loss=1.53 avg=2.02\n",
      "[565 | 20824.03] loss=1.80 avg=2.02\n",
      "[566 | 20855.86] loss=1.67 avg=2.01\n",
      "[567 | 20890.40] loss=1.64 avg=2.01\n",
      "[568 | 20923.52] loss=1.38 avg=2.00\n",
      "[569 | 20959.63] loss=1.87 avg=2.00\n",
      "[570 | 20993.51] loss=1.75 avg=2.00\n",
      "[571 | 21028.30] loss=1.13 avg=1.99\n",
      "[572 | 21063.15] loss=1.97 avg=1.99\n",
      "[573 | 21096.34] loss=1.58 avg=1.99\n",
      "[574 | 21131.11] loss=1.50 avg=1.98\n",
      "[575 | 21165.71] loss=1.53 avg=1.98\n",
      "[576 | 21199.81] loss=1.38 avg=1.97\n",
      "[577 | 21232.89] loss=1.42 avg=1.97\n",
      "[578 | 21267.56] loss=1.63 avg=1.96\n",
      "[579 | 21302.37] loss=1.62 avg=1.96\n",
      "[580 | 21337.20] loss=1.79 avg=1.96\n",
      "[581 | 21371.65] loss=1.55 avg=1.95\n",
      "[582 | 21405.66] loss=1.69 avg=1.95\n",
      "[583 | 21441.01] loss=1.42 avg=1.95\n",
      "[584 | 21474.07] loss=1.65 avg=1.94\n",
      "[585 | 21510.74] loss=1.79 avg=1.94\n",
      "[586 | 21545.25] loss=1.59 avg=1.94\n",
      "[587 | 21577.70] loss=1.75 avg=1.94\n",
      "[588 | 21613.51] loss=1.71 avg=1.93\n",
      "[589 | 21645.98] loss=1.21 avg=1.93\n",
      "[590 | 21681.40] loss=1.43 avg=1.92\n",
      "[591 | 21714.91] loss=1.54 avg=1.92\n",
      "[592 | 21749.57] loss=1.48 avg=1.91\n",
      "[593 | 21785.06] loss=1.53 avg=1.91\n",
      "[594 | 21818.50] loss=1.83 avg=1.91\n",
      "[595 | 21852.04] loss=1.63 avg=1.91\n",
      "[596 | 21886.95] loss=1.15 avg=1.90\n",
      "[597 | 21921.04] loss=1.46 avg=1.89\n",
      "[598 | 21953.73] loss=1.47 avg=1.89\n",
      "[599 | 21990.13] loss=1.42 avg=1.89\n",
      "[600 | 22024.10] loss=1.80 avg=1.88\n",
      "======== SAMPLE 1 ========\n",
      " detain a man, that's a sin\n",
      "He shall be held amercement twenty years:\n",
      "I would he be a free man. I see here\n",
      "An empty penitent bower: be content.\n",
      "This man will rob a poor gentleman of his\n",
      "capillature, his mind with idle looks and\n",
      "wasping, an empty sword: do not do this;\n",
      "do at once, and save your honour in\n",
      "doing; for 'tis a fault to do it quickly.\n",
      "\n",
      "GLOUCESTER:\n",
      "He doth mock me, methinks,\n",
      "With this insulting tongue: I did inform\n",
      "Myself his nature.\n",
      "\n",
      "BUCKINGHAM:\n",
      "You rogue peers, your hearts are made\n",
      "Against a king, and your heads against his\n",
      "barbed points. You seek aught but to make\n",
      "A mortal wound with him whose contempt you boldly\n",
      "May infect with your own true hate?\n",
      "\n",
      "GLOUCESTER:\n",
      "Nay, rather than fearing him, you dread him\n",
      "To a measure quite beyond your compass:\n",
      "Do not despair; you must, worse enemies\n",
      "Than he whose angry point most accaunts\n",
      "And fires a fiery coal with an ignoble flame\n",
      "From his reach to scape a king with jealous rage:\n",
      "Leave him, then, your father, for he doth\n",
      "Work for this bastard; and, in that sad time\n",
      "When men seem dull, dull they're ever\n",
      "More moved to do good: what they fear,\n",
      "Their hearts are made against the bastard,\n",
      "And with a piteous cry strike him dead,\n",
      "Which will enjoin another bastard\n",
      "To this new-heal'd name. Mark what 'our fear\n",
      "'fates have to do with thee!'\n",
      "And so, in brief, old Warwick and young Harry\n",
      "Will be revenged for this unholy deed,\n",
      "Which Edward, in an apotheosis\n",
      "Of gorgeous, in a marriage to a man,\n",
      "Was framed to Gaunt.\n",
      "\n",
      "WESTMORELAND:\n",
      "So let our cause be heard; and that's\n",
      "A cause of Gaunt and Edward.\n",
      "\n",
      "GLOUCESTER:\n",
      "Why, so let it be!\n",
      "\n",
      "BUCKINGHAM:\n",
      "And now King of England, and my true liege,\n",
      "My liege, the day-boughs full as old,\n",
      "And of the finest array of all the blood\n",
      "We have, and blood as well as armour, do supply,\n",
      "Are to be hasten: therefore, like precious ones,\n",
      "Be it as you are, according to the hour.\n",
      "\n",
      "GLOUCESTER:\n",
      "But, by the holy cur, let's not stand so long\n",
      "To discourse of what we do: both our counsel\n",
      "Doth depend upon the truth of late,\n",
      "And time hath not permitted us to speak:\n",
      "Thus, in this most sudden sudden shock, God and all\n",
      "Kings, make known to the whole world we\n",
      "How I was subdued at Ely, how\n",
      "Lossed at Gloucester's death, and how haply\n",
      "My loving lord to-day\n",
      "May be brought to greater judgment, I mean,\n",
      "This business that shall then stand between us\n",
      "And lasting comfort till our friends shall\n",
      "Be ruled by us as lords not theirs.\n",
      "\n",
      "BUCKINGHAM:\n",
      "My lord, the news is most of late the like\n",
      "Perplexed, my lord, at the utterance\n",
      "Of Gaunt:'\n",
      "And scarce can you speak to-day, while we\n",
      "Commit ourselves to look into this mess;\n",
      "And, being now much moved, do affright us\n",
      "To march on to battle.\n",
      "\n",
      "KING RICHARD III:\n",
      "Our new king, the blessed Lord Salisbury,\n",
      "The ministers of his holy zeal\n",
      "Upon Warwick to-morrow in fight,\n",
      "He lieth once more subdued, and withal\n",
      "Enacts what stands to-morrow in rage,\n",
      "Like to a rage he does it, like a fool,\n",
      "To lose the field; but redeems us all,\n",
      "Though this morning's fire be fresh. To-morrow\n",
      "We'll build our walls hereon; and then, to-morrow,\n",
      "Let nature bring forth new and replenish'd sepulchres\n",
      "Of ferns and barren places: then, to-morrow,\n",
      "And with alacrity, we'll drive Edward from his wall,\n",
      "And with victory set his gates up in force,\n",
      "Where none but God and Edward knows\n",
      "Ape to-morrow.\n",
      "\n",
      "GLOUCESTER:\n",
      "My lord, he means, with many a man,\n",
      "That keeps all against himself?\n",
      "\n",
      "KING RICHARD III:\n",
      "Ay, and waits upon him\n",
      "For such a purpose as that.\n",
      "\n",
      "GLOUCESTER:\n",
      "What is he, that waits upon him\n",
      "For such a purpose as that?\n",
      "\n",
      "KING RICH\n",
      "\n",
      "[601 | 22127.75] loss=1.48 avg=1.88\n",
      "[602 | 22160.77] loss=1.53 avg=1.88\n",
      "[603 | 22200.97] loss=1.82 avg=1.88\n",
      "[604 | 22234.47] loss=1.66 avg=1.87\n",
      "[605 | 22269.02] loss=1.08 avg=1.87\n",
      "[606 | 22303.08] loss=1.40 avg=1.86\n",
      "[607 | 22337.76] loss=1.53 avg=1.86\n",
      "[608 | 22371.33] loss=1.54 avg=1.86\n",
      "[609 | 22406.35] loss=1.49 avg=1.85\n",
      "[610 | 22438.46] loss=1.71 avg=1.85\n",
      "[611 | 22474.17] loss=1.05 avg=1.84\n",
      "[612 | 22509.41] loss=1.17 avg=1.84\n",
      "[613 | 22542.02] loss=1.22 avg=1.83\n",
      "[614 | 22578.24] loss=1.59 avg=1.83\n",
      "[615 | 22613.51] loss=1.50 avg=1.82\n",
      "[616 | 22646.73] loss=1.64 avg=1.82\n",
      "[617 | 22680.24] loss=1.31 avg=1.82\n",
      "[618 | 22715.57] loss=1.44 avg=1.81\n",
      "[619 | 22749.68] loss=1.12 avg=1.81\n",
      "[620 | 22784.50] loss=1.63 avg=1.80\n",
      "[621 | 22817.47] loss=1.34 avg=1.80\n",
      "[622 | 22851.75] loss=1.70 avg=1.80\n",
      "[623 | 22887.81] loss=1.19 avg=1.79\n",
      "[624 | 22920.61] loss=1.52 avg=1.79\n",
      "[625 | 22955.76] loss=1.54 avg=1.79\n",
      "[626 | 22990.70] loss=1.38 avg=1.78\n",
      "[627 | 23023.40] loss=1.24 avg=1.78\n",
      "[628 | 23058.07] loss=1.16 avg=1.77\n",
      "[629 | 23092.15] loss=0.98 avg=1.76\n",
      "[630 | 23127.61] loss=1.59 avg=1.76\n",
      "[631 | 23160.51] loss=1.53 avg=1.76\n",
      "[632 | 23195.20] loss=1.70 avg=1.76\n",
      "[633 | 23231.09] loss=1.32 avg=1.75\n",
      "[634 | 23266.15] loss=1.17 avg=1.75\n",
      "[635 | 23300.22] loss=1.31 avg=1.74\n",
      "[636 | 23334.83] loss=1.47 avg=1.74\n",
      "[637 | 23369.72] loss=1.34 avg=1.74\n",
      "[638 | 23402.09] loss=1.12 avg=1.73\n",
      "[639 | 23437.63] loss=1.25 avg=1.73\n",
      "[640 | 23472.43] loss=1.24 avg=1.72\n",
      "[641 | 23506.22] loss=1.32 avg=1.72\n",
      "[642 | 23539.76] loss=1.37 avg=1.71\n",
      "[643 | 23574.38] loss=1.23 avg=1.71\n",
      "[644 | 23609.11] loss=1.10 avg=1.70\n",
      "[645 | 23642.04] loss=1.14 avg=1.70\n",
      "[646 | 23678.61] loss=1.22 avg=1.69\n",
      "[647 | 23711.62] loss=1.37 avg=1.69\n",
      "[648 | 23744.87] loss=1.06 avg=1.68\n",
      "[649 | 23779.37] loss=1.25 avg=1.68\n",
      "[650 | 23813.18] loss=1.63 avg=1.68\n",
      "[651 | 23847.94] loss=1.05 avg=1.67\n",
      "[652 | 23881.53] loss=1.50 avg=1.67\n",
      "[653 | 23915.16] loss=1.63 avg=1.67\n",
      "[654 | 23951.06] loss=1.19 avg=1.66\n",
      "[655 | 23985.21] loss=1.47 avg=1.66\n",
      "[656 | 24019.10] loss=0.89 avg=1.66\n",
      "[657 | 24054.83] loss=1.32 avg=1.65\n",
      "[658 | 24090.15] loss=1.47 avg=1.65\n",
      "[659 | 24134.55] loss=1.20 avg=1.65\n",
      "[660 | 24167.24] loss=1.18 avg=1.64\n",
      "[661 | 24199.30] loss=1.32 avg=1.64\n",
      "[662 | 24234.54] loss=0.95 avg=1.63\n",
      "[663 | 24267.48] loss=1.17 avg=1.63\n",
      "[664 | 24303.27] loss=1.46 avg=1.62\n",
      "[665 | 24337.20] loss=1.25 avg=1.62\n",
      "[666 | 24370.12] loss=1.42 avg=1.62\n",
      "[667 | 24407.11] loss=1.53 avg=1.62\n",
      "[668 | 24440.94] loss=1.30 avg=1.61\n",
      "[669 | 24473.88] loss=1.56 avg=1.61\n",
      "[670 | 24508.66] loss=0.87 avg=1.61\n",
      "[671 | 24545.25] loss=1.04 avg=1.60\n",
      "[672 | 24578.91] loss=1.26 avg=1.60\n",
      "[673 | 24612.13] loss=1.65 avg=1.60\n",
      "[674 | 24647.38] loss=1.04 avg=1.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[675 | 24682.11] loss=1.18 avg=1.59\n",
      "[676 | 24716.28] loss=1.42 avg=1.59\n",
      "[677 | 24750.67] loss=1.06 avg=1.58\n",
      "[678 | 24787.08] loss=1.52 avg=1.58\n",
      "[679 | 24820.80] loss=1.08 avg=1.58\n",
      "[680 | 24854.80] loss=1.23 avg=1.57\n",
      "[681 | 24889.97] loss=1.14 avg=1.57\n",
      "[682 | 24923.22] loss=1.02 avg=1.56\n",
      "[683 | 24957.98] loss=1.22 avg=1.56\n",
      "[684 | 24993.86] loss=1.06 avg=1.55\n",
      "[685 | 25027.80] loss=1.29 avg=1.55\n",
      "[686 | 25062.52] loss=1.33 avg=1.55\n",
      "[687 | 25097.68] loss=1.23 avg=1.55\n",
      "[688 | 25132.25] loss=1.06 avg=1.54\n",
      "[689 | 25166.49] loss=1.24 avg=1.54\n",
      "[690 | 25200.76] loss=1.46 avg=1.54\n",
      "[691 | 25234.37] loss=1.29 avg=1.53\n",
      "[692 | 25270.01] loss=0.72 avg=1.53\n",
      "[693 | 25304.06] loss=0.98 avg=1.52\n",
      "[694 | 25337.14] loss=1.48 avg=1.52\n",
      "[695 | 25371.31] loss=1.08 avg=1.52\n",
      "[696 | 25407.07] loss=1.29 avg=1.51\n",
      "[697 | 25441.50] loss=0.82 avg=1.51\n",
      "[698 | 25476.12] loss=0.90 avg=1.50\n",
      "[699 | 25510.82] loss=1.17 avg=1.50\n",
      "[700 | 25544.38] loss=1.20 avg=1.49\n",
      "======== SAMPLE 1 ========\n",
      " they have.\n",
      "\n",
      "PETRUCHIO:\n",
      "I am an't scared, then.\n",
      "\n",
      "TRANIO:\n",
      "Ay, sir, so will I to his house: I'll straightway betimes\n",
      "open the paper upon Baptista's advancement.\n",
      "\n",
      "PETRUCHIO:\n",
      "The ink is dry! the paper to his house is dry.\n",
      "\n",
      "HORTENSIO:\n",
      "Well, sir, as the case may be, may my fortune lighten\n",
      "On me, that, I hope, my father will be a prince!\n",
      "The queen's son is born--I do not know that name, but\n",
      "I dread her--and the other defects,\n",
      "Poor props, debuts, declines, and so\n",
      "Upon her descent be easy.\n",
      "\n",
      "LUCENTIO:\n",
      "I do beseech your highness, see that presently:\n",
      "I do but fear my voice.\n",
      "\n",
      "LUCENTIO:\n",
      "Dowl you, quoth he my father. There comes your brother\n",
      "At the house of this Capulet.\n",
      "\n",
      "TRANIO:\n",
      "How! at the door! What is the matter?\n",
      "\n",
      "VINCENTIO:\n",
      "What am I, sir?\n",
      "\n",
      "VINCENTIO:\n",
      "What can I?\n",
      "\n",
      "VINCENTIO:\n",
      "What can I, sir?\n",
      "\n",
      "VINCENTIO:\n",
      "Thieves, sir! thieves!\n",
      "\n",
      "GREMIO:\n",
      "What has he stolen from my shop-keep?\n",
      "\n",
      "HORTENSIO:\n",
      "No, sir; he's been stole from shopkeepers,\n",
      "And robbed here on Padua. Come, let us go:\n",
      "I'll hold us fast, and deal in bad faith.\n",
      "\n",
      "BIONDELLO:\n",
      "What is the matter?\n",
      "\n",
      "GREMIO:\n",
      "Grumio, come forth; come forth; come forth.\n",
      "\n",
      "VINCENTIO:\n",
      "Father, I come forth; come forth!\n",
      "\n",
      "BIONDELLO:\n",
      "What! father, I come forth; come forth!\n",
      "\n",
      "PETRUCHIO:\n",
      "Go, get aboard, get aboard; come forth!\n",
      "\n",
      "BAPTISTA:\n",
      "Why, sir, you are a fool to undertake this action!\n",
      "\n",
      "PETRUCHIO:\n",
      "Grumio, gimme some light.\n",
      "\n",
      "GREMIO:\n",
      "What!\n",
      "\n",
      "VINCENTIO:\n",
      "If thou be a man, God forbid,\n",
      "Put forth a hand, as I am about to speak,\n",
      "But push away that hand with that foot,\n",
      "Or else--God forbid!--I'ld hurl down stone\n",
      "And pluck thy gimme from thy throat,\n",
      "As though that hand were against thy life,\n",
      "Or my dear bride in question.\n",
      "\n",
      "PETRUCHIO:\n",
      "Now, Signior Gremio, give me this painting.\n",
      "Why, what now, what now! here's Gremio's house.\n",
      "\n",
      "LUCENTIO:\n",
      "What, how now, what now! my table, my table!\n",
      "\n",
      "PETRUCHIO:\n",
      "Out on thy table! Gentles, canst thou not put\n",
      "That holy rag in question? Gentles, will you lose\n",
      "Your only son, his spite given up to summer?\n",
      "Ah, sirrah, what has he to lose by this?\n",
      "\n",
      "GRUMIO:\n",
      "What, will my father lose his spite? Go not thus;\n",
      "I'll visit his boy.\n",
      "\n",
      "GRUMIO:\n",
      "O temporum non facitum, non necessari\n",
      "Gregis faciune tuum non sicut honours.\n",
      "\n",
      "BIANCA:\n",
      "What, say you will?\n",
      "\n",
      "KATHARINA:\n",
      "I have wanted of him for a month\n",
      "And haven to him not been forthcoming in love.\n",
      "\n",
      "GRUMIO:\n",
      "Then he must lose two of his best strokes,\n",
      "And lose three of them with one go.\n",
      "\n",
      "KATHARINA:\n",
      "O temporum non facitum, non honours!\n",
      "\n",
      "GRUMIO:\n",
      "The honours do attend, as so, the forget.\n",
      "\n",
      "KATHARINA:\n",
      "Gardener, I'll stop my strokes and be gone.\n",
      "\n",
      "GRUMIO:\n",
      "But I will visit your mark, can you? O cousin,\n",
      "A word of tricks will be your afternoon.\n",
      "\n",
      "HORTENSIO:\n",
      "Why, cousin, I love you not.\n",
      "\n",
      "KATHARINA:\n",
      "Well, well, bestow your grief on me.\n",
      "\n",
      "GRUMIO:\n",
      "Tut, tut!\n",
      "\n",
      "KATHARINA:\n",
      "Gardener, I will be there; I have but one wish,\n",
      "To see you soon met.\n",
      "\n",
      "GRUMIO:\n",
      "Where's my father?\n",
      "\n",
      "KATHARINA:\n",
      "Here, when I am met\n",
      "\n",
      "[701 | 25646.31] loss=1.36 avg=1.49\n",
      "[702 | 25679.53] loss=1.11 avg=1.49\n",
      "[703 | 25720.88] loss=0.99 avg=1.48\n",
      "[704 | 25755.43] loss=1.21 avg=1.48\n",
      "[705 | 25788.14] loss=1.02 avg=1.48\n",
      "[706 | 25824.22] loss=1.71 avg=1.48\n",
      "[707 | 25858.37] loss=0.94 avg=1.47\n",
      "[708 | 25891.79] loss=1.06 avg=1.47\n",
      "[709 | 25926.84] loss=1.14 avg=1.47\n",
      "[710 | 25959.84] loss=0.97 avg=1.46\n",
      "[711 | 25994.43] loss=0.83 avg=1.46\n",
      "[712 | 26029.39] loss=0.86 avg=1.45\n",
      "[713 | 26061.51] loss=0.87 avg=1.44\n",
      "[714 | 26098.01] loss=1.03 avg=1.44\n",
      "[715 | 26132.73] loss=1.23 avg=1.44\n",
      "[716 | 26166.30] loss=1.20 avg=1.43\n",
      "[717 | 26202.85] loss=0.88 avg=1.43\n",
      "[718 | 26237.30] loss=0.64 avg=1.42\n",
      "[719 | 26269.88] loss=1.17 avg=1.42\n",
      "[720 | 26305.44] loss=0.79 avg=1.41\n",
      "[721 | 26339.76] loss=1.04 avg=1.41\n",
      "[722 | 26373.53] loss=0.84 avg=1.40\n",
      "[723 | 26408.05] loss=1.21 avg=1.40\n",
      "[724 | 26442.23] loss=1.23 avg=1.40\n",
      "[725 | 26478.16] loss=1.10 avg=1.40\n",
      "[726 | 26511.77] loss=0.85 avg=1.39\n",
      "[727 | 26546.30] loss=1.03 avg=1.39\n",
      "[728 | 26582.34] loss=1.06 avg=1.38\n",
      "[729 | 26615.78] loss=1.19 avg=1.38\n",
      "[730 | 26650.99] loss=1.17 avg=1.38\n",
      "[731 | 26685.71] loss=0.81 avg=1.37\n",
      "[732 | 26722.33] loss=1.07 avg=1.37\n",
      "[733 | 26755.70] loss=0.60 avg=1.36\n",
      "[734 | 26790.36] loss=1.03 avg=1.36\n",
      "[735 | 26826.62] loss=1.16 avg=1.36\n",
      "[736 | 26860.11] loss=0.82 avg=1.35\n",
      "[737 | 26896.35] loss=0.82 avg=1.35\n",
      "[738 | 26931.74] loss=0.94 avg=1.34\n",
      "[739 | 26966.64] loss=0.82 avg=1.34\n",
      "[740 | 27000.73] loss=0.93 avg=1.33\n",
      "[741 | 27035.06] loss=0.68 avg=1.33\n",
      "[742 | 27071.17] loss=1.07 avg=1.33\n",
      "[743 | 27103.61] loss=0.90 avg=1.32\n",
      "[744 | 27137.94] loss=0.75 avg=1.32\n",
      "[745 | 27173.16] loss=0.93 avg=1.31\n",
      "[746 | 27207.91] loss=1.05 avg=1.31\n",
      "[747 | 27241.08] loss=1.11 avg=1.31\n",
      "[748 | 27275.81] loss=1.06 avg=1.30\n",
      "[749 | 27310.66] loss=0.76 avg=1.30\n",
      "[750 | 27343.25] loss=0.90 avg=1.29\n",
      "[751 | 27378.17] loss=1.01 avg=1.29\n",
      "[752 | 27412.45] loss=0.92 avg=1.29\n",
      "[753 | 27448.17] loss=0.95 avg=1.28\n",
      "[754 | 27481.11] loss=0.68 avg=1.28\n",
      "[755 | 27514.58] loss=0.69 avg=1.27\n",
      "[756 | 27551.45] loss=0.96 avg=1.27\n",
      "[757 | 27584.84] loss=0.87 avg=1.27\n",
      "[758 | 27620.35] loss=0.99 avg=1.26\n",
      "[759 | 27655.36] loss=0.78 avg=1.26\n",
      "[760 | 27688.90] loss=0.72 avg=1.25\n",
      "[761 | 27723.22] loss=0.83 avg=1.25\n",
      "[762 | 27760.40] loss=1.21 avg=1.25\n",
      "[763 | 27795.28] loss=0.91 avg=1.24\n",
      "[764 | 27828.16] loss=1.04 avg=1.24\n",
      "[765 | 27863.41] loss=1.20 avg=1.24\n",
      "[766 | 27898.45] loss=0.94 avg=1.24\n",
      "[767 | 27932.41] loss=0.79 avg=1.23\n",
      "[768 | 27966.23] loss=0.92 avg=1.23\n",
      "[769 | 28001.51] loss=0.81 avg=1.23\n",
      "[770 | 28036.61] loss=0.70 avg=1.22\n",
      "[771 | 28070.30] loss=1.06 avg=1.22\n",
      "[772 | 28105.05] loss=1.39 avg=1.22\n",
      "[773 | 28138.18] loss=0.82 avg=1.22\n",
      "[774 | 28173.61] loss=0.67 avg=1.21\n",
      "[775 | 28207.30] loss=0.70 avg=1.21\n",
      "[776 | 28241.81] loss=0.71 avg=1.20\n",
      "[777 | 28277.59] loss=0.84 avg=1.20\n",
      "[778 | 28310.51] loss=0.92 avg=1.20\n",
      "[779 | 28346.98] loss=0.61 avg=1.19\n",
      "[780 | 28380.98] loss=0.86 avg=1.19\n",
      "[781 | 28415.20] loss=0.88 avg=1.18\n",
      "[782 | 28449.29] loss=0.88 avg=1.18\n",
      "[783 | 28484.08] loss=1.08 avg=1.18\n",
      "[784 | 28520.27] loss=0.93 avg=1.18\n",
      "[785 | 28552.69] loss=0.92 avg=1.17\n",
      "[786 | 28588.76] loss=1.18 avg=1.17\n",
      "[787 | 28623.43] loss=0.51 avg=1.17\n",
      "[788 | 28658.63] loss=0.76 avg=1.16\n",
      "[789 | 28692.71] loss=0.88 avg=1.16\n",
      "[790 | 28728.32] loss=0.88 avg=1.16\n",
      "[791 | 28762.95] loss=0.90 avg=1.16\n",
      "[792 | 28795.71] loss=0.83 avg=1.15\n",
      "[793 | 28830.92] loss=0.90 avg=1.15\n",
      "[794 | 28864.81] loss=0.91 avg=1.15\n",
      "[795 | 28901.15] loss=0.59 avg=1.14\n",
      "[796 | 28934.68] loss=0.64 avg=1.14\n",
      "[797 | 28969.70] loss=0.78 avg=1.13\n",
      "[798 | 29005.78] loss=0.59 avg=1.13\n",
      "[799 | 29038.10] loss=0.83 avg=1.12\n",
      "[800 | 29076.08] loss=0.66 avg=1.12\n",
      "======== SAMPLE 1 ========\n",
      "Out to her.\n",
      "\n",
      "LADY CAPULET:\n",
      "She doth fear me; and all my kindred are but fearful.\n",
      "Weep, my lord, I say true; but the creature having\n",
      "A child by you, a lawful grandam,--a grandam\n",
      "That by the queen my mother is a king,--it\n",
      "Would be most unnatural for you or me to ne'er\n",
      "See her so close up and to affect the grace\n",
      "Of the court. For a minute, I pray you,\n",
      "You give this boy too much credit. 'Tis like\n",
      "She is the prettiest of the litter, the prettiest\n",
      "That ever came by the nose; the one with\n",
      "A gentle caress and gentle play, which was never\n",
      "gracious to her; never tender--never--a word changed.\n",
      "She's the prettiest of the litter, she said? O' my, she was.\n",
      "\n",
      "CAPULET:\n",
      "O, but she said so unto another servant,\n",
      "A muriner born before Baptista I was.\n",
      "She had something to say to this boy, she said.\n",
      "O' my, a thousand things were muriners' sounds;\n",
      "To thrust their strings at the same time their intonations\n",
      "Dreamed upon was to make a thousand soldiers their cymbals.\n",
      "But was this the girl? was she young? or was she\n",
      "just? O' my, she was't she that young!\n",
      "\n",
      "BRAKENEY:\n",
      "She was.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Why, what news?\n",
      "\n",
      "BRYANT:\n",
      "Richmond.\n",
      "\n",
      "KING RICHARD III:\n",
      "Cold tears for me wash them from my cheeks\n",
      "Like to the parting of hare and rheum\n",
      "That reacheth in the winter wind. I thank God we shall never\n",
      "Be together, though the water be of some comfort\n",
      "And kindred able to breathe again,\n",
      "For it is now almost dry. I have pray'd too much,\n",
      "And see no sign of God's graces: though I look on the earth,\n",
      "I cannot but see cold tears welling within,\n",
      "Whiskey-diving in a bank, or a rough bank\n",
      "Broken with knots passing it: some part of me\n",
      "Knead with hope and some part with horror\n",
      "Till time had left me, and all buted when I saw\n",
      "Dry tears welling up in the cheeks of sad men,\n",
      "Stoning with grief the rotten gossips: all this had\n",
      "No merit or moral merit, but only\n",
      "The appearance of grief and the want of meaning\n",
      "To chide and scorn the task.\n",
      "\n",
      "KING RICHARD III:\n",
      "My head is fair from an unseasonable day\n",
      "Driving away the shears, the strings, and the runs,\n",
      "I wander through the green forest of snowy nights\n",
      "To count the herds, the sheep, and the goats that went\n",
      "Before me: every horse that trods\n",
      "Upon the way was a greeting worthy of his master.\n",
      "I dare assure you, my manly judge,\n",
      "Did not in the book of Kabbalah the saying,\n",
      "'The wise man will shew his master when he goes'?\n",
      "Did not the enduringly good old John the Baptist\n",
      "Go back to his horse to beg for his bawd in the field\n",
      "From a woman he love so? Did not the goslingers go\n",
      "There to buy the innocent off their wives?\n",
      "Did not the orange-tongued lawyers go\n",
      "There to buy ayop and his misdeeds in the sessions\n",
      "From an old orator? Did not the mountain bears go\n",
      "There to torment and eat and be pined for?\n",
      "Did not the seas-angel overseer go\n",
      "There to pluck and tear the hoary and dying brain\n",
      "Of an innocent adulteress?\n",
      "Did not the magician and the minister\n",
      "Of ancient magic make an end of this wicked\n",
      "Hour and night in their satanic satanic plots?\n",
      "Did not the spirit of serpents smear\n",
      "The aged monuments with their venomous spines\n",
      "And spilt the deadly venom of this helpless soul\n",
      "From the fiery venom of this damned creature?\n",
      "O, but a fire-breather might have done this?\n",
      "But I, a woman of faith and justice,\n",
      "I bear the cross of Paulina, and I,\n",
      "A serpentine likeness of Jove's damnation,\n",
      "Gosling the pale-faced-God to chastise her.\n",
      "This state of things, this rage, this fury, this division\n",
      "Must become a height, a height that will pierce\n",
      "Fell more riotous and bloody than the fain\n",
      "That would have clung to peace and cozens.\n",
      "The first to fall, that is the first that must appear\n",
      "Must be the villain, and I must and shall go.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[801 | 29180.64] loss=0.72 avg=1.12\n",
      "[802 | 29218.28] loss=0.63 avg=1.11\n",
      "[803 | 29264.98] loss=0.47 avg=1.11\n",
      "[804 | 29296.13] loss=0.67 avg=1.10\n",
      "[805 | 29329.54] loss=0.76 avg=1.10\n",
      "[806 | 29367.15] loss=0.54 avg=1.09\n",
      "[807 | 29398.71] loss=0.84 avg=1.09\n",
      "[808 | 29433.27] loss=0.67 avg=1.09\n",
      "[809 | 29469.17] loss=0.60 avg=1.08\n",
      "[810 | 29503.50] loss=1.54 avg=1.08\n",
      "[811 | 29537.64] loss=0.67 avg=1.08\n",
      "[812 | 29575.14] loss=0.82 avg=1.08\n",
      "[813 | 29609.68] loss=0.69 avg=1.07\n",
      "[814 | 29644.52] loss=1.08 avg=1.07\n",
      "[815 | 29680.87] loss=0.77 avg=1.07\n",
      "[816 | 29715.52] loss=0.78 avg=1.07\n",
      "[817 | 29751.54] loss=0.73 avg=1.06\n",
      "[818 | 29787.61] loss=0.64 avg=1.06\n",
      "[819 | 29823.46] loss=0.79 avg=1.06\n",
      "[820 | 29859.36] loss=0.80 avg=1.06\n",
      "[821 | 29895.11] loss=0.71 avg=1.05\n",
      "[822 | 29932.66] loss=0.77 avg=1.05\n",
      "[823 | 29966.52] loss=1.11 avg=1.05\n",
      "[824 | 30000.91] loss=0.61 avg=1.05\n",
      "[825 | 30039.63] loss=0.83 avg=1.04\n",
      "[826 | 30074.28] loss=0.86 avg=1.04\n",
      "[827 | 30109.82] loss=0.50 avg=1.04\n",
      "[828 | 30146.09] loss=1.02 avg=1.04\n",
      "[829 | 30182.14] loss=0.70 avg=1.03\n",
      "[830 | 30216.91] loss=0.55 avg=1.03\n",
      "[831 | 30251.62] loss=1.17 avg=1.03\n",
      "[832 | 30286.47] loss=0.67 avg=1.03\n",
      "[833 | 30322.63] loss=0.75 avg=1.02\n",
      "[834 | 30359.36] loss=0.76 avg=1.02\n",
      "[835 | 30394.45] loss=0.63 avg=1.02\n",
      "[836 | 30432.43] loss=0.92 avg=1.02\n",
      "[837 | 30467.87] loss=0.44 avg=1.01\n",
      "[838 | 30503.98] loss=0.55 avg=1.00\n",
      "[839 | 30541.29] loss=0.88 avg=1.00\n",
      "[840 | 30575.76] loss=1.06 avg=1.00\n",
      "[841 | 30609.72] loss=0.72 avg=1.00\n",
      "[842 | 30646.48] loss=0.80 avg=1.00\n",
      "[843 | 30681.59] loss=0.83 avg=1.00\n",
      "[844 | 30717.37] loss=0.54 avg=0.99\n",
      "[845 | 30751.96] loss=0.70 avg=0.99\n",
      "[846 | 30790.62] loss=0.53 avg=0.99\n",
      "[847 | 30826.39] loss=0.61 avg=0.98\n",
      "[848 | 30860.47] loss=0.75 avg=0.98\n",
      "[849 | 30898.90] loss=0.64 avg=0.98\n",
      "[850 | 30933.57] loss=1.09 avg=0.98\n",
      "[851 | 30968.08] loss=0.46 avg=0.97\n",
      "[852 | 31004.75] loss=0.73 avg=0.97\n",
      "[853 | 31040.70] loss=0.45 avg=0.96\n",
      "[854 | 31077.15] loss=0.62 avg=0.96\n",
      "[855 | 31111.62] loss=0.95 avg=0.96\n",
      "[856 | 31149.51] loss=0.65 avg=0.96\n",
      "[857 | 31184.48] loss=0.78 avg=0.96\n",
      "[858 | 31220.62] loss=0.58 avg=0.95\n",
      "[859 | 31259.29] loss=0.61 avg=0.95\n",
      "[860 | 31293.54] loss=0.72 avg=0.95\n",
      "[861 | 31328.95] loss=0.77 avg=0.94\n",
      "[862 | 31365.25] loss=0.44 avg=0.94\n",
      "[863 | 31400.83] loss=0.47 avg=0.93\n",
      "[864 | 31436.00] loss=0.65 avg=0.93\n",
      "[865 | 31471.04] loss=0.57 avg=0.93\n",
      "[866 | 31508.17] loss=0.61 avg=0.93\n",
      "[867 | 31545.65] loss=0.69 avg=0.92\n",
      "[868 | 31581.29] loss=0.50 avg=0.92\n",
      "[869 | 31619.71] loss=0.71 avg=0.92\n",
      "[870 | 31655.90] loss=0.75 avg=0.91\n",
      "[871 | 31691.69] loss=0.65 avg=0.91\n",
      "[872 | 31726.77] loss=0.73 avg=0.91\n",
      "[873 | 31764.12] loss=0.74 avg=0.91\n",
      "[874 | 31800.18] loss=0.54 avg=0.91\n",
      "[875 | 31834.98] loss=0.40 avg=0.90\n",
      "[876 | 31875.46] loss=0.46 avg=0.90\n",
      "[877 | 31910.08] loss=0.70 avg=0.89\n",
      "[878 | 31946.69] loss=0.43 avg=0.89\n",
      "[879 | 31984.66] loss=0.64 avg=0.89\n",
      "[880 | 32017.54] loss=0.53 avg=0.88\n",
      "[881 | 32054.90] loss=0.56 avg=0.88\n",
      "[882 | 32089.40] loss=0.70 avg=0.88\n",
      "[883 | 32125.94] loss=0.64 avg=0.88\n",
      "[884 | 32163.66] loss=0.53 avg=0.87\n",
      "[885 | 32198.80] loss=0.42 avg=0.87\n",
      "[886 | 32241.90] loss=0.46 avg=0.86\n",
      "[887 | 32285.73] loss=0.62 avg=0.86\n",
      "[888 | 32316.57] loss=0.70 avg=0.86\n",
      "[889 | 32356.05] loss=0.84 avg=0.86\n",
      "[890 | 32390.08] loss=0.57 avg=0.86\n",
      "[891 | 32427.32] loss=0.65 avg=0.85\n",
      "[892 | 32463.82] loss=0.42 avg=0.85\n",
      "[893 | 32498.89] loss=0.40 avg=0.85\n",
      "[894 | 32537.44] loss=0.49 avg=0.84\n",
      "[895 | 32574.18] loss=0.54 avg=0.84\n",
      "[896 | 32611.38] loss=0.66 avg=0.84\n",
      "[897 | 32646.99] loss=0.63 avg=0.83\n",
      "[898 | 32683.77] loss=0.67 avg=0.83\n",
      "[899 | 32721.93] loss=0.60 avg=0.83\n",
      "[900 | 32755.75] loss=0.69 avg=0.83\n",
      "======== SAMPLE 1 ========\n",
      "all, if it please you\n",
      "Be ruled by the precedent of your own proceeding.\n",
      "\n",
      "LUCIO:\n",
      "Come, come, my lord.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "An if it please you, my noble duke,\n",
      "Either for his own correction or for his peace\n",
      "To have a gentle father, as he hath done,\n",
      "Do you hear what Lord Angelo told me?\n",
      "\n",
      "LUCIO:\n",
      "Lord, how like to be gentle?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "It is a bitter ghost to hear it. How would you\n",
      "Mistake the reproof of my gentle request\n",
      "For the pleasing of the sovereign.\n",
      "\n",
      "LUCIO:\n",
      "Most so, most untruth; but suppose the matter;\n",
      "Were there but one prince whom you could wish\n",
      "For your brother's speedy happiness, as he\n",
      "Should so far deserve it?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "If the matter were but one, my gracious lord,\n",
      "Would I should mistreat your grace's fair offer,\n",
      "Which so far was accursed for his freedom,\n",
      "But so dear for his sake: not that I\n",
      "Like your particular order nor that your request\n",
      "Me to mope, but that it may have harm\n",
      "Upon itself, which it has much praise\n",
      "For doing, which your grace bids it be\n",
      "Or do not do. But, so please you, I beseech you\n",
      "Look past it: you wish my brother good night.\n",
      "\n",
      "LUCIO:\n",
      "I will beseech you, look forward to it:\n",
      "He's of about the same condition, I know,\n",
      "As our brother Clarence was: but, lest you look\n",
      "In vain, lest 'twere not so, behold my hand,\n",
      "Which tells you tender tidings of good news,\n",
      "And hand to hand with my master's sad-faced news,\n",
      "As yours,--for here I have told you news,\n",
      "As told by myself, your sister and me,--\n",
      "These news indeed is; for there is a devilish mercy\n",
      "In such a case: but come; we'll deal with good,\n",
      "And bad, good;'tis nothing but this.\n",
      "Come, cousin, we'll work it.\n",
      "\n",
      "ARCHIDAMUS:\n",
      "No, I'll not go: your cousin must come\n",
      "To know him, and not be tied to't.\n",
      "\n",
      "MONTAGUE:\n",
      "What, shall we unward him, and deal with him?\n",
      "\n",
      "MONTAGUE:\n",
      "Let me get him,\n",
      "Because he is a brother unto my lord.\n",
      "\n",
      "ARCHIDAMUS:\n",
      "So would I,\n",
      "Even to my loins, to lord the queen of heaven.\n",
      "\n",
      "MONTAGUE:\n",
      "Look to the record,\n",
      "And, behold, the king made my brother king.\n",
      "\n",
      "ARCHIDAMUS:\n",
      "What said my brother?\n",
      "\n",
      "HASTINGS:\n",
      "My lord's speak'd of him; and had I been he was slain.\n",
      "\n",
      "ARCHIDAMUS:\n",
      "So would he be so.\n",
      "\n",
      "HASTINGS:\n",
      "My lord's talk of him; had I been he was slain,\n",
      "I would have talk'd of him.\n",
      "\n",
      "ARCHIDAMUS:\n",
      "Talk not, look at the cross-examination:\n",
      "I do it not thinkfully, for it concerns\n",
      "No thought that I may harbour, but therefore by me\n",
      "Proceed not so foolishly.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Say not he is slain, dispatch: I'll have you\n",
      "as witnesses for my eye, by sooth.\n",
      "\n",
      "ARCHIDAMUS:\n",
      "One hour, my lord?\n",
      "\n",
      "HASTINGS:\n",
      "Why, now 'tis midnight.\n",
      "\n",
      "ARCHIDAMUS:\n",
      "The day iss yet?\n",
      "\n",
      "HASTINGS:\n",
      "Not yet, my lord.\n",
      "\n",
      "ARCHIDAMUS:\n",
      "And you not yet, my lord?\n",
      "\n",
      "HASTINGS:\n",
      "Not yet, my lord.\n",
      "\n",
      "ARCHIDAMUS:\n",
      "Send me to the devil, I say?\n",
      "\n",
      "HASTINGS:\n",
      "Sir, dispatch.\n",
      "\n",
      "ARCHIDAMUS:\n",
      "Sir, I say he is toward Montague.\n",
      "\n",
      "MONTAGUE:\n",
      "\n",
      "ARCHIDAMUS:\n",
      "Where is that devil, if he come so near\n",
      "As you look into his eye?\n",
      "\n",
      "HASTINGS:\n",
      "At thy house.\n",
      "\n",
      "ARCHIDAMUS:\n",
      "Thy house, that lies between us,--\n",
      "Where two horns can meet,--taill-descent,--will\n",
      "Throw light upon thy devilery,\n",
      "And bring thee to light as devils?\n",
      "\n",
      "HASTINGS:\n",
      "Call it naught.\n",
      "\n",
      "ARCHIDAMUS:\n",
      "Thy bed.\n",
      "\n",
      "MONTAGUE:\n",
      "\n",
      "ARCHIDAMUS:\n",
      "Thy pillow.\n",
      "\n",
      "\n",
      "\n",
      "[901 | 32862.84] loss=0.53 avg=0.83\n",
      "[902 | 32946.12] loss=0.49 avg=0.82\n",
      "[903 | 32986.57] loss=0.60 avg=0.82\n",
      "[904 | 33018.24] loss=0.67 avg=0.82\n",
      "[905 | 33054.30] loss=0.49 avg=0.82\n",
      "[906 | 33103.35] loss=0.46 avg=0.81\n",
      "[907 | 33153.87] loss=0.64 avg=0.81\n",
      "[908 | 33210.67] loss=0.49 avg=0.81\n",
      "[909 | 33263.31] loss=0.63 avg=0.81\n",
      "[910 | 33304.33] loss=0.59 avg=0.80\n",
      "[911 | 33351.04] loss=0.47 avg=0.80\n",
      "[912 | 33395.37] loss=0.72 avg=0.80\n",
      "[913 | 33435.36] loss=0.33 avg=0.79\n",
      "[914 | 33495.01] loss=0.47 avg=0.79\n",
      "[915 | 33539.93] loss=0.53 avg=0.79\n",
      "[916 | 33573.72] loss=0.57 avg=0.79\n",
      "[917 | 33615.96] loss=0.51 avg=0.78\n",
      "[918 | 33658.07] loss=0.43 avg=0.78\n",
      "[919 | 33717.88] loss=0.45 avg=0.78\n",
      "interrupted\n",
      "Saving checkpoint/run1/model-919\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(sess, dataset, steps, model_name, model_dir, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, checkpoint_dir, sample_every, sample_length, sample_num, multi_gpu, save_every, print_every, max_checkpoints, use_memory_saving_gradients, only_train_transformer_layers, optimizer, overwrite)\u001b[0m\n\u001b[1;32m    336\u001b[0m                     sess.run(\n\u001b[0;32m--> 337\u001b[0;31m                         opt_compute, feed_dict={context: sample_batch()})\n\u001b[0m\u001b[1;32m    338\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mv_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_summary\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-13c5305f15e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m               \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m               steps=1000)   # steps is max number of training steps\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(sess, dataset, steps, model_name, model_dir, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, checkpoint_dir, sample_every, sample_length, sample_num, multi_gpu, save_every, print_every, max_checkpoints, use_memory_saving_gradients, only_train_transformer_layers, optimizer, overwrite)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'interrupted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36msave\u001b[0;34m()\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             global_step=counter-1)\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1174\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1175\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sentgen_tf_115/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "file_name = \"shakespeare.txt\"\n",
    "if not os.path.isfile(file_name):\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    data = requests.get(url)\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(data.text)\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess,\n",
    "              file_name,\n",
    "              model_name=model_name,\n",
    "              steps=1000)   # steps is max number of training steps\n",
    "\n",
    "gpt2.generate(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets for each class to finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../data/input/groundtruth/'\n",
    "OUTPUT_DIR = '../data/input/groundtruth/'\n",
    "\n",
    "CORPUS_DIR = os.path.join(OUTPUT_DIR, 'corpus')\n",
    "VECTORIZED_DIR = os.path.join(OUTPUT_DIR, 'vectorized_trainset')\n",
    "\n",
    "text_col_header = 'text'\n",
    "label_col_header = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_train_file = os.path.join(INPUT_DIR, 'speechact_train.csv')\n",
    "df_train = pd.read_csv(ip_train_file)\n",
    "df_train = df_train.astype({text_col_header: str, label_col_header: int})\n",
    "\n",
    "ip_test_file = os.path.join(INPUT_DIR, 'speechact_test.csv')\n",
    "df_test = pd.read_csv(ip_test_file)\n",
    "df_test = df_test.astype({text_col_header: str, label_col_header: int})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create individual files for each class to finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15645, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train_0 = df_train.loc[df_train[label_col_header]==0]\n",
    "print(df_train_0.shape)\n",
    "\n",
    "class_csv_file = os.path.join(INPUT_DIR, 'class0_statement_text.txt')\n",
    "with open(class_csv_file, 'w') as f_ip:\n",
    "    for text in df_train_0[text_col_header]:\n",
    "        f_ip.write(str(text)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11629, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train_1 = df_train.loc[df_train[label_col_header]==1]\n",
    "print(df_train_1.shape)\n",
    "\n",
    "class_csv_file = os.path.join(INPUT_DIR, 'class1_interrogative_text.txt')\n",
    "with open(class_csv_file, 'w') as f_ip:\n",
    "    for text in df_train_1[text_col_header]:\n",
    "        f_ip.write(str(text)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9825, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train_2 = df_train.loc[df_train[label_col_header]==2]\n",
    "print(df_train_2.shape)\n",
    "\n",
    "class_csv_file = os.path.join(INPUT_DIR, 'class2_imperative_text.txt')\n",
    "with open(class_csv_file, 'w') as f_ip:\n",
    "    for text in df_train_2[text_col_header]:\n",
    "        f_ip.write(str(text)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    # antonyms = []\n",
    "\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "                \n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_synonym(sentence, word):\n",
    "    \n",
    "    augmented_sentences = []\n",
    "    augmented_sentences.append(sentence)\n",
    "    \n",
    "    sentence_words = sentence.split(' ')\n",
    "    \n",
    "    if word in sentence_words:\n",
    "        \n",
    "        synonyms = get_synonyms(word)\n",
    "    \n",
    "        for synonym in synonyms:\n",
    "            reg_ex = r'\\b'+word+r'\\b'\n",
    "            new_setence = re.sub(reg_ex, synonym, sentence)\n",
    "        \n",
    "            augmented_sentences.append(new_setence)\n",
    "        \n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_augmentation(sentence):\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    words = sentence.split(' ')\n",
    "    \n",
    "    for word in words:\n",
    "        # print('---------------------\\n', word)\n",
    "        new_sentences = replace_synonym(sentence, word)\n",
    "        # print(new_sentences)\n",
    "        sentences.extend(new_sentences)\n",
    "        \n",
    "    return list(set(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmented_dataframe(sentence, label):\n",
    "    \n",
    "    augmented_sentences = synonym_augmentation(sentence)\n",
    "    labels = [label] * len(augmented_sentences)\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(augmented_sentences, labels)), columns=[text_col_header, label_col_header])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_augmentation_withoutstopwords(sentence):\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    words = sentence.split(' ')\n",
    "    \n",
    "    for word in words:\n",
    "        # print('---------------------\\n', word)\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_sentences = replace_synonym(sentence, word)\n",
    "            # print(new_sentences)\n",
    "            sentences.extend(new_sentences)\n",
    "        \n",
    "    return list(set(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketers representing industry raid manufactu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rolling stone magazine said richards had creat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>most designs are inflated through pyrotechnic ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>please focus on the article s topic not the title</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other types of arthropod produce silk most not...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  marketers representing industry raid manufactu...      0\n",
       "1  rolling stone magazine said richards had creat...      0\n",
       "2  most designs are inflated through pyrotechnic ...      0\n",
       "3  please focus on the article s topic not the title      2\n",
       "4  other types of arthropod produce silk most not...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17556/37099] Total:2024222 Added rows:371\r"
     ]
    }
   ],
   "source": [
    "df_train_augmented = pd.DataFrame(columns=[text_col_header, label_col_header])\n",
    "for index, row in df_train.iterrows():\n",
    "    \n",
    "    sentence = row[text_col_header]\n",
    "    label = row[label_col_header]\n",
    "    \n",
    "    df = get_augmented_dataframe(sentence, label)\n",
    "    \n",
    "    df_train_augmented = pd.concat([df_train_augmented, df], axis=0, sort=False)\n",
    "    \n",
    "    print('['+str(index)+'/'+str(len(df_train))+'] Total:'+str(len(df_train_augmented))+\\\n",
    "          ' Added rows:'+str(len(df)), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_file = os.path.join(OUTPUT_DIR,'speechact_augmented_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_augmented.to_csv(op_file, index=False, header=[text_col_header, label_col_header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_augmented = pd.read_csv(op_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_augmented.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_stopwords=False):\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # print(text, len(text), end ='\\n')\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W',' ', text)\n",
    "    text = re.sub(' \\d+', ' ', text)\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        words = text.split(' ')\n",
    "        words = [w.strip() for w in words if w not in stopwords.words('english')]\n",
    "        text = ' '.join(words)\n",
    "    text = text.strip()\n",
    "        \n",
    "    # print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_based_on_occurences(text_list, min_occurences=1):\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    alltext = ''\n",
    "    for text in text_list:\n",
    "        alltext += text\n",
    "        \n",
    "    wordlist = alltext.split()\n",
    "\n",
    "    word_freq = dict(Counter(wordlist))\n",
    "    # print(word_freq)\n",
    "    \n",
    "    for word,freq in word_freq.items():\n",
    "        if (freq >= min_occurences):\n",
    "            corpus.append(word)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df_data, text_col_header, remove_stopwords):\n",
    "    \n",
    "    df_data[text_col_header] = df_data[text_col_header].apply(lambda x: preprocess_text(x,remove_stopwords))\n",
    "    df_data.drop(df_data[df_data[text_col_header] == ''].index, inplace=True)\n",
    "    \n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating multiple corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Length  37099\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords = False\n",
    "text_col_header = 'text'\n",
    "\n",
    "df_train_stopword = df_train.copy()\n",
    "df_train_stopword = preprocess_df(df_train_stopword, text_col_header, remove_stopwords)\n",
    "\n",
    "# df_test_stopword = df_test.copy()\n",
    "# df_test_stopword = preprocess_df(df_test_stopword, text_col_header, remove_stopwords)\n",
    "\n",
    "corpus_stopword = df_train_stopword[text_col_header].values\n",
    "\n",
    "print('Corpus Length ', len(corpus_stopword))\n",
    "\n",
    "corpus_file = os.path.join(CORPUS_DIR, 'corpus_stopword.pkl')\n",
    "with open(corpus_file, 'wb') as f_op:\n",
    "    pkl.dump(corpus_stopword, f_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Length  37020\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords = True\n",
    "text_col_header = 'text'\n",
    "\n",
    "df_train_nostopword = df_train.copy()\n",
    "df_train_nostopword = preprocess_df(df_train_nostopword, text_col_header, remove_stopwords)\n",
    "\n",
    "# df_test_nostopword = df_test.copy()\n",
    "# df_test_nostopword = preprocess_df(df_test_nostopword, text_col_header, remove_stopwords)\n",
    "\n",
    "corpus_nostopword = df_train_nostopword[text_col_header].values\n",
    "\n",
    "print('Corpus Length ', len(corpus_nostopword))\n",
    "\n",
    "corpus_file = os.path.join(CORPUS_DIR, 'corpus_nostopword.pkl')\n",
    "with open(corpus_file, 'wb') as f_op:\n",
    "    pkl.dump(corpus_nostopword, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create optional corpus with only minimum occurring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_min_occurrences = remove_words_based_on_occurences(corpus_nostopword, min_occurences=2)\n",
    "\n",
    "corpus_file = os.path.join(CORPUS_DIR, 'corpus_min_occurrences_2.pkl')\n",
    "with open(corpus_file, 'wb') as f_op:\n",
    "    pkl.dump(corpus_min_occurrences, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization - Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are 3 corpus that are to be vectorized\n",
    "  <li> Bag of words </li>\n",
    "  <li> Tf-IDF </li>\n",
    "  <li> Word embeddings - Glove </li>\n",
    "  <li> BERT embeddings </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data train: (37099, 34154)\n",
      "Shape of the label train: (37099, 1)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_stopword)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_countvector_stopword.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_countvector_stopword.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_countvector_stopword.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)\n",
    "\n",
    "# data_test = vectorizer.transform(df_test_stopword[text_col_header])\n",
    "# print('Shape of the data train:',data_test.shape)\n",
    "# label_test = np.array(df_test_stopword[label_col_header])\n",
    "# label_test = label_test.reshape((len(label_test), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data train: (37099, 34014)\n",
      "Shape of the label train: (37099, 1)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_nostopword)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_countvector_nostopword.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_countvector_nostopword.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_countvector_nostopword.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - minimum occurrence words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data train: (37099, 16518)\n",
      "Shape of the label train: (37099, 1)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_min_occurrences)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_countvector_minoccurences_2.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_countvector_minoccurences.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_countvector_minoccurences.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentegen_tf_115",
   "language": "python",
   "name": "sentegen_tf_115"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
