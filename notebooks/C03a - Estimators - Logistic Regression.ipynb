{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../data/input/groundtruth/'\n",
    "OUTPUT_DIR = '../data/input/groundtruth/'\n",
    "\n",
    "CORPUS_DIR = os.path.join(OUTPUT_DIR, 'corpus')\n",
    "VECTORIZED_DIR = os.path.join(OUTPUT_DIR, 'vectorized_trainset')\n",
    "\n",
    "text_col_header = 'text'\n",
    "label_col_header = 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading test and train data to keep them ready for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read Train data\n",
    "ip_train_file = os.path.join(INPUT_DIR, 'speechact_train.csv')\n",
    "df_train = pd.read_csv(ip_train_file)\n",
    "df_train = df_train.astype({text_col_header: str, label_col_header: int})\n",
    "\n",
    "# Read Test data\n",
    "ip_test_file = os.path.join(INPUT_DIR, 'speechact_test.csv')\n",
    "df_test = pd.read_csv(ip_test_file)\n",
    "df_test = df_test.astype({text_col_header: str, label_col_header: int})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "<li> Input - Vectorized Data train </li>\n",
    "<li> Input - Vectorized Label train </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter Vector with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vectorized train data and train label\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_countvector_stopword.pkl')\n",
    "with open(vectorized_train_data_file, 'rb') as f_ip:\n",
    "    data_train = pkl.load(f_ip)\n",
    "    \n",
    "train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_countvector_stopword.pkl')\n",
    "with open(train_label_file, 'rb') as f_ip:\n",
    "    label_train = pkl.load(f_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vectorizer to transform the test data\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_countvector_stopword.pkl')\n",
    "with open(vector_file, 'rb') as f_ip:\n",
    "    vectorizer = pkl.load(f_ip)\n",
    "\n",
    "data_test = vectorizer.transform(df_test[text_col_header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Logistic Regression\n",
    "estimator = LogisticRegression(max_iter=1000)\n",
    "estimator.fit(data_train, np.ravel(label_train, order='C'))\n",
    "predictions_test = estimator.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9875737890509179\n",
      "Test Accuracy 0.9392991913746631\n",
      "F1 macro Score:  0.9383546832567413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95      3911\n",
      "           1       0.94      0.93      0.94      2908\n",
      "           2       0.92      0.94      0.93      2456\n",
      "\n",
      "    accuracy                           0.94      9275\n",
      "   macro avg       0.94      0.94      0.94      9275\n",
      "weighted avg       0.94      0.94      0.94      9275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "## Accuracy Measure\n",
    "print('Train Accuracy', estimator.score(data_train, df_train[label_col_header]))\n",
    "print('Test Accuracy', estimator.score(data_test, df_test[label_col_header]))\n",
    "\n",
    "# F1\n",
    "f1_measure = f1_score(df_test[label_col_header], predictions_test, average='macro')\n",
    "print('F1 macro Score: ', f1_measure)\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_test[label_col_header], predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store estimator and run sample sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESTIMATOR_DIR = '../models/'\n",
    "estimator_file = os.path.join(ESTIMATOR_DIR, 'estimator_speechact.pkl')\n",
    "\n",
    "with open(estimator_file, 'wb') as f_ip:\n",
    "    pkl.dump(estimator, f_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # print(text, end ='')\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W',' ', text)\n",
    "    text = re.sub(' \\d+', ' ', text)\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "        \n",
    "    words = text.split(' ')\n",
    "    words = [w.strip() for w in words]\n",
    "    \n",
    "    text = ' '.join(words)\n",
    "    text = text.strip()\n",
    "        \n",
    "    # print(text)\n",
    "    return text\n",
    "\n",
    "def text_to_speechact(text, estimator_file, embedding_file, corpus_file):\n",
    "\n",
    "    # text = preprocess_text(sample_sentence).strip()\n",
    "    text = preprocess_text(text).strip()\n",
    "    \n",
    "    textlist = []\n",
    "    ret_result = 'None'\n",
    "    \n",
    "    if text:\n",
    "        \n",
    "        # The following code can be pushed into the initialization\n",
    "        # portion of the application to improve efficiency\n",
    "        # ------ BEGIN ----------\n",
    "\n",
    "        # Read the embedding\n",
    "        with open(embedding_file, 'rb') as f_ip:\n",
    "            vectorizer = pkl.load(f_ip)\n",
    "            \n",
    "        # Read the estimator\n",
    "        with open(estimator_file, 'rb') as f_ip:\n",
    "            estimator = pkl.load(f_ip)\n",
    "            \n",
    "        # -------- END -----------\n",
    "        \n",
    "        # Vectorize the input text\n",
    "        textlist.append(text)\n",
    "        text_vector = vectorizer.transform(textlist)\n",
    "        \n",
    "        # Estimate the predicted value\n",
    "        pred = estimator.predict(text_vector)\n",
    "        pred_val = pred[0]\n",
    "\n",
    "        # Return the prediction in the form of a string\n",
    "        # Efficiency: Return as integers and convert in the last responsible moment\n",
    "        if pred_val == 0:\n",
    "            ret_result = 'statement'\n",
    "        elif pred_val == 1:\n",
    "            ret_result = 'interrogative'\n",
    "        elif pred_val == 2:\n",
    "            ret_result = 'imperative'\n",
    "            \n",
    "    return ret_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interrogative\n",
      "imperative\n"
     ]
    }
   ],
   "source": [
    "def speechact_wrapper(sample_sentence):\n",
    "    \n",
    "    estimator_file = '../models/estimator_speechact.pkl'\n",
    "    embedding_file = '../data/input/groundtruth/corpus/vector_countvector_stopword.pkl'\n",
    "    \n",
    "    speech_act = text_to_speechact(text=sample_sentence, \n",
    "                      estimator_file=estimator_file, \n",
    "                      embedding_file=embedding_file, \n",
    "                      corpus_file=None)\n",
    "    \n",
    "    return speech_act\n",
    "\n",
    "print(speechact_wrapper('What is the name of the person'))\n",
    "print(speechact_wrapper('Go away'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter Vector with no stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vectorized train data and train label\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_countvector_nostopword.pkl')\n",
    "with open(vectorized_train_data_file, 'rb') as f_ip:\n",
    "    data_train = pkl.load(f_ip)\n",
    "    \n",
    "train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_countvector_nostopword.pkl')\n",
    "with open(train_label_file, 'rb') as f_ip:\n",
    "    label_train = pkl.load(f_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vectorizer to transform the test data\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_countvector_nostopword.pkl')\n",
    "with open(vector_file, 'rb') as f_ip:\n",
    "    vectorizer = pkl.load(f_ip)\n",
    "\n",
    "data_test = vectorizer.transform(df_test[text_col_header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Logistic Regression\n",
    "estimator = LogisticRegression(max_iter=1000)\n",
    "estimator.fit(data_train, np.ravel(label_train, order='C'))\n",
    "predictions_test = estimator.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9605380198927195\n",
      "Test Accuracy 0.8659838274932614\n",
      "F1 macro Score:  0.8661387567725706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89      3911\n",
      "           1       0.79      0.85      0.82      2908\n",
      "           2       0.91      0.88      0.89      2456\n",
      "\n",
      "    accuracy                           0.87      9275\n",
      "   macro avg       0.87      0.87      0.87      9275\n",
      "weighted avg       0.87      0.87      0.87      9275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "## Accuracy Measure\n",
    "print('Train Accuracy', estimator.score(data_train, df_train[label_col_header]))\n",
    "print('Test Accuracy', estimator.score(data_test, df_test[label_col_header]))\n",
    "\n",
    "# F1\n",
    "f1_measure = f1_score(df_test[label_col_header], predictions_test, average='macro')\n",
    "print('F1 macro Score: ', f1_measure)\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_test[label_col_header], predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter Vector with minimum occurences of words (min_occurences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vectorized train data and train label\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'train_data_countvector_minoccurences_2.pkl')\n",
    "with open(vectorized_train_data_file, 'rb') as f_ip:\n",
    "    data_train = pkl.load(f_ip)\n",
    "    \n",
    "train_label_file = os.path.join(VECTORIZED_DIR, 'train_label_countvector_minoccurences_2.pkl')\n",
    "with open(train_label_file, 'rb') as f_ip:\n",
    "    label_train = pkl.load(f_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vectorizer to transform the test data\n",
    "vector_file = os.path.join(CORPUS_DIR, 'vector_countvector_minoccurences_2.pkl')\n",
    "with open(vector_file, 'rb') as f_ip:\n",
    "    vectorizer = pkl.load(f_ip)\n",
    "\n",
    "data_test = vectorizer.transform(df_test[text_col_header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Logistic Regression\n",
    "estimator = LogisticRegression(max_iter=1000)\n",
    "estimator.fit(data_train, np.ravel(label_train, order='C'))\n",
    "predictions_test = estimator.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9453624086902612\n",
      "Test Accuracy 0.8636118598382749\n",
      "F1 macro Score:  0.863375437946586\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89      3911\n",
      "           1       0.78      0.85      0.81      2908\n",
      "           2       0.90      0.87      0.89      2456\n",
      "\n",
      "    accuracy                           0.86      9275\n",
      "   macro avg       0.86      0.86      0.86      9275\n",
      "weighted avg       0.87      0.86      0.86      9275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "## Accuracy Measure\n",
    "print('Train Accuracy', estimator.score(data_train, df_train[label_col_header]))\n",
    "print('Test Accuracy', estimator.score(data_test, df_test[label_col_header]))\n",
    "\n",
    "# F1\n",
    "f1_measure = f1_score(df_test[label_col_header], predictions_test, average='macro')\n",
    "print('F1 macro Score: ', f1_measure)\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_test[label_col_header], predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentgen",
   "language": "python",
   "name": "sentgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
