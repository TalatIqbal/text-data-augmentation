{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../data/input/groundtruth/'\n",
    "OUTPUT_DIR = '../data/input/groundtruth/'\n",
    "\n",
    "CORPUS_DIR = os.path.join(OUTPUT_DIR, 'corpus')\n",
    "VECTORIZED_DIR = os.path.join(OUTPUT_DIR, 'vectorized_trainset')\n",
    "\n",
    "text_col_header = 'text'\n",
    "label_col_header = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_train_file = os.path.join(INPUT_DIR, 'speechact_train.csv')\n",
    "df_train = pd.read_csv(ip_train_file)\n",
    "df_train = df_train.astype({text_col_header: str, label_col_header: int})\n",
    "\n",
    "ip_test_file = os.path.join(INPUT_DIR, 'speechact_test.csv')\n",
    "df_test = pd.read_csv(ip_test_file)\n",
    "df_test = df_test.astype({text_col_header: str, label_col_header: int})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    # antonyms = []\n",
    "\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "                \n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_synonym(sentence, word):\n",
    "    \n",
    "    augmented_sentences = []\n",
    "    augmented_sentences.append(sentence)\n",
    "    \n",
    "    sentence_words = sentence.split(' ')\n",
    "    \n",
    "    if word in sentence_words:\n",
    "        \n",
    "        synonyms = get_synonyms(word)\n",
    "    \n",
    "        for synonym in synonyms:\n",
    "            reg_ex = r'\\b'+word+r'\\b'\n",
    "            new_setence = re.sub(reg_ex, synonym, sentence)\n",
    "        \n",
    "            augmented_sentences.append(new_setence)\n",
    "        \n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_augmentation(sentence):\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    words = sentence.split(' ')\n",
    "    \n",
    "    for word in words:\n",
    "        # print('---------------------\\n', word)\n",
    "        new_sentences = replace_synonym(sentence, word)\n",
    "        # print(new_sentences)\n",
    "        sentences.extend(new_sentences)\n",
    "        \n",
    "    return list(set(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmented_dataframe(sentence, label):\n",
    "    \n",
    "    augmented_sentences = synonym_augmentation(sentence)\n",
    "    labels = [label] * len(augmented_sentences)\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(augmented_sentences, labels)), columns=[text_col_header, label_col_header])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_augmentation_withoutstopwords(sentence):\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    words = sentence.split(' ')\n",
    "    \n",
    "    for word in words:\n",
    "        # print('---------------------\\n', word)\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_sentences = replace_synonym(sentence, word)\n",
    "            # print(new_sentences)\n",
    "            sentences.extend(new_sentences)\n",
    "        \n",
    "    return list(set(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketers representing industry raid manufactu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rolling stone magazine said richards had creat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>most designs are inflated through pyrotechnic ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>please focus on the article s topic not the title</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other types of arthropod produce silk most not...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  marketers representing industry raid manufactu...      0\n",
       "1  rolling stone magazine said richards had creat...      0\n",
       "2  most designs are inflated through pyrotechnic ...      0\n",
       "3  please focus on the article s topic not the title      2\n",
       "4  other types of arthropod produce silk most not...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37098/37099] Total:4268921 Added rows:438\r"
     ]
    }
   ],
   "source": [
    "df_train_augmented = pd.DataFrame(columns=[text_col_header, label_col_header])\n",
    "for index, row in df_train.iterrows():\n",
    "    \n",
    "    sentence = row[text_col_header]\n",
    "    label = row[label_col_header]\n",
    "    \n",
    "    df = get_augmented_dataframe(sentence, label)\n",
    "    \n",
    "    df_train_augmented = pd.concat([df_train_augmented, df], axis=0, sort=False)\n",
    "    \n",
    "    print('['+str(index)+'/'+str(len(df_train))+'] Total:'+str(len(df_train_augmented))+\\\n",
    "          ' Added rows:'+str(len(df)), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_file = os.path.join(OUTPUT_DIR,'speechact_augmented_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_augmented.to_csv(op_file, index=False, header=[text_col_header, label_col_header])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the data is augmented text train file is available, run from the below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_augmented = pd.read_csv(op_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37099, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_augmented.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4268921, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_stopwords=False):\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # print(text, len(text), end ='\\n')\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W',' ', text)\n",
    "    text = re.sub(' \\d+', ' ', text)\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        words = text.split(' ')\n",
    "        words = [w.strip() for w in words if w not in stopwords.words('english')]\n",
    "        text = ' '.join(words)\n",
    "    text = text.strip()\n",
    "        \n",
    "    # print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_based_on_occurences(text_list, min_occurences=1):\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    alltext = ''\n",
    "    for text in text_list:\n",
    "        alltext += text\n",
    "        \n",
    "    wordlist = alltext.split()\n",
    "\n",
    "    word_freq = dict(Counter(wordlist))\n",
    "    # print(word_freq)\n",
    "    \n",
    "    for word,freq in word_freq.items():\n",
    "        if (freq >= min_occurences):\n",
    "            corpus.append(word)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df_data, text_col_header, remove_stopwords):\n",
    "    \n",
    "    df_data[text_col_header] = df_data[text_col_header].apply(lambda x: preprocess_text(x,remove_stopwords))\n",
    "    df_data.drop(df_data[df_data[text_col_header] == ''].index, inplace=True)\n",
    "    \n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating multiple corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Length  4268921\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords = False\n",
    "text_col_header = 'text'\n",
    "\n",
    "df_train_stopword = df_train.copy()\n",
    "df_train_stopword = preprocess_df(df_train_stopword, text_col_header, remove_stopwords)\n",
    "\n",
    "# df_test_stopword = df_test.copy()\n",
    "# df_test_stopword = preprocess_df(df_test_stopword, text_col_header, remove_stopwords)\n",
    "\n",
    "corpus_stopword = df_train_stopword[text_col_header].values\n",
    "\n",
    "print('Corpus Length ', len(corpus_stopword))\n",
    "\n",
    "corpus_file = os.path.join(CORPUS_DIR, 'aug_corpus_stopword.pkl')\n",
    "with open(corpus_file, 'wb') as f_op:\n",
    "    pkl.dump(corpus_stopword, f_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Length  4268577\n"
     ]
    }
   ],
   "source": [
    "remove_stopwords = True\n",
    "text_col_header = 'text'\n",
    "\n",
    "df_train_nostopword = df_train.copy()\n",
    "df_train_nostopword = preprocess_df(df_train_nostopword, text_col_header, remove_stopwords)\n",
    "\n",
    "# df_test_nostopword = df_test.copy()\n",
    "# df_test_nostopword = preprocess_df(df_test_nostopword, text_col_header, remove_stopwords)\n",
    "\n",
    "corpus_nostopword = df_train_nostopword[text_col_header].values\n",
    "\n",
    "print('Corpus Length ', len(corpus_nostopword))\n",
    "\n",
    "corpus_file = os.path.join(CORPUS_DIR, 'aug_corpus_nostopword.pkl')\n",
    "with open(corpus_file, 'wb') as f_op:\n",
    "    pkl.dump(corpus_nostopword, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create optional corpus with only minimum occurring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_min_occurrences = remove_words_based_on_occurences(corpus_nostopword, min_occurences=2)\n",
    "\n",
    "corpus_file = os.path.join(CORPUS_DIR, 'aug_corpus_min_occurrences_2.pkl')\n",
    "with open(corpus_file, 'wb') as f_op:\n",
    "    pkl.dump(corpus_min_occurrences, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization - Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are 3 corpus that are to be vectorized\n",
    "  <li> Bag of words </li>\n",
    "  <li> Tf-IDF </li>\n",
    "  <li> Word embeddings - Glove </li>\n",
    "  <li> BERT embeddings </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data train: (4268921, 61054)\n",
      "Shape of the label train: (4268921, 1)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_stopword)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'aug_vector_countvector_stopword.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'aug_train_data_countvector_stopword.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'aug_train_label_countvector_stopword.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)\n",
    "\n",
    "# data_test = vectorizer.transform(df_test_stopword[text_col_header])\n",
    "# print('Shape of the data train:',data_test.shape)\n",
    "# label_test = np.array(df_test_stopword[label_col_header])\n",
    "# label_test = label_test.reshape((len(label_test), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data train: (4268921, 60914)\n",
      "Shape of the label train: (4268921, 1)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_nostopword)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'aug_vector_countvector_nostopword.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'aug_train_data_countvector_nostopword.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'aug_train_label_countvector_nostopword.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus - minimum occurrence words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data train: (4268921, 188794)\n",
      "Shape of the label train: (4268921, 1)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_min_occurrences)\n",
    "\n",
    "# Write the vectorizer itself\n",
    "vector_file = os.path.join(CORPUS_DIR, 'aug_vector_countvector_minoccurences_2.pkl')\n",
    "with open(vector_file, 'wb') as f_op:\n",
    "    pkl.dump(vectorizer, f_op)\n",
    "\n",
    "data_train = vectorizer.transform(df_train_stopword[text_col_header])\n",
    "print('Shape of the data train:',data_train.shape)\n",
    "label_train = np.array(df_train_stopword[label_col_header])\n",
    "label_train = label_train.reshape((len(label_train), 1))\n",
    "print('Shape of the label train:',label_train.shape)\n",
    "\n",
    "vectorized_train_data_file = os.path.join(VECTORIZED_DIR, 'aug_train_data_countvector_minoccurences.pkl')\n",
    "with open(vectorized_train_data_file, 'wb') as f_op:\n",
    "    pkl.dump(data_train, f_op)\n",
    "    \n",
    "vectorized_train_label_file = os.path.join(VECTORIZED_DIR, 'aug_train_label_countvector_minoccurences.pkl')\n",
    "with open(vectorized_train_label_file, 'wb') as f_op:\n",
    "    pkl.dump(label_train, f_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentgen",
   "language": "python",
   "name": "sentgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
